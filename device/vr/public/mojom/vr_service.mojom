// Copyright 2015 The Chromium Authors
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

module device.mojom;

import "device/gamepad/public/mojom/gamepad.mojom";
import "device/vr/public/mojom/xr_session.mojom";
import "mojo/public/mojom/base/big_buffer.mojom";
import "mojo/public/mojom/base/time.mojom";
import "gpu/ipc/common/mailbox_holder.mojom";
import "gpu/ipc/common/sync_token.mojom";
import "ui/display/mojom/display.mojom";
import "ui/gfx/geometry/mojom/geometry.mojom";
import "ui/gfx/mojom/gpu_fence_handle.mojom";
import "ui/gfx/mojom/transform.mojom";

//
// WebXR interfaces
//

// Note on terminology: unless otherwise noted, all poses passed across mojo are
// expressed in device space, aka mojo space, aka world space.

enum XRHandedness {
  NONE = 0,
  LEFT = 1,
  RIGHT = 2
};

enum XRTargetRayMode {
  GAZING = 1,
  POINTING = 2,
  TAPPING = 3
};

// These values are persisted to logs. Entries should not be renumbered and
// numeric values should never be reused.
// This enum corresponds to 'XRFeatureRequestStatus' in
// tools/metrics/histograms/enums.xml
enum XRSessionFeatureRequestStatus {
  kNotRequested = 0,
  kRequired = 1,
  kOptionalAccepted = 2,
  kOptionalRejected = 3,
};

// These enum names come from the WebXR spec:
// https://immersive-web.github.io/webxr-ar-module/#enumdef-xrenvironmentblendmode
// When presenting content to the XR device, the XR Compositor MUST apply the
// appropriate blend technique to combine virtual pixels with the
// real-world environment.
enum XREnvironmentBlendMode {
  kOpaque = 1,
  kAlphaBlend = 2,
  kAdditive = 3,
};

// These enum names come from the WebXR spec:
// https://immersive-web.github.io/webxr-ar-module/#enumdef-xrinteractionmode
// The interactionMode attribute describes the best space
// (according to the user agent) for the application to draw interactive
// UI for the current session.
enum XRInteractionMode {
  kScreenSpace = 1,
  kWorldSpace = 2,
};

struct XRDepthConfig {
  XRDepthUsage depth_usage;
  XRDepthDataFormat depth_data_format;
};

// This structure contains a description of the device's active configuration
// for the session being established.
struct XRSessionDeviceConfig {
  // The default scale that should be applied to the native framebuffer size
  // unless overridden by the developer.
  float default_framebuffer_scale = 1.0;

  // Information about all the views in the device at the time the session was
  // created. Views may change on any frame, so the session should inspect
  // subsequent views in XRFrameData on each frame.
  array<XRView> views;

  // Indicates whether the device supports dynamic viewport scaling via
  // XRView.requestViewportScale which results in UpdateLayerBounds mojo calls.
  // This is opt-in to avoid problems with devices that assume default bounds,
  // this would result in broken rendering due to inconsistency between the
  // drawn and displayed viewports.
  bool supports_viewport_scaling;

  // Indicates whether we should enable anti-aliasing for WebGL layers. Value
  // comes from the underlying XR runtime.
  bool enable_anti_aliasing = true;

  // If the session supports depth-sensing API, the depth configuration must be
  // non-null.
  XRDepthConfig? depth_configuration;
};

// This structure contains all the mojo interfaces for different kinds of
// XRSession. The only interface required by all sessions is the
// XRFrameDataProvider. It must always be present. Other interfaces are set as
// appropriate based on the session creation options. (for example, an immersive
// session ought to have a XRPresentationConnection to submit the frames to the
// immersive environment).
// The XRSessionClient receiver must be fulfilled for the session to get
// information about the device it is connected to, such as focus and blur
// events, changes to view or stage parameters, or exit present calls initiated
// by the device.
// This struct is created in the device process, then manipulated in the browser
// process before being handed off to the renderer process.
struct XRSession {
  pending_remote<XRFrameDataProvider> data_provider;
  // TODO(http://crbug.com/842025) Move where the client_receiver gets set to the
  // device process then mark this as non-optional.
  pending_receiver<XRSessionClient>? client_receiver;
  XRPresentationConnection? submit_frame_sink;

  // Represents the set of requested features that are both supported by the
  // runtime, and have been consented to by the user.
  array<XRSessionFeature> enabled_features;

  // Description of the device's active configuration.
  XRSessionDeviceConfig device_config;

  XREnvironmentBlendMode enviroment_blend_mode;
  XRInteractionMode interaction_mode;
};

// This structure contains the infomation and interfaces needed to create a two
// way connection between the renderer and a device to synchronize and submit
// frames to a sink outside of Chrome.
struct XRPresentationConnection {
  pending_remote<XRPresentationProvider> provider;
  pending_receiver<XRPresentationClient> client_receiver;
  XRPresentationTransportOptions transport_options;
};

struct XRInputSourceDescription {
  XRTargetRayMode target_ray_mode;
  XRHandedness handedness;

  // Pointer ray pose in input device space. The pointer space origin represents
  // the point on the input device where the ray originates, and the ray travels
  // along the pointer space negative-Z direction.
  // For controllers, the input device space is grip space (defined by the
  // controller's position). For screen input, the input device space is
  // equivalent to viewer space, and the ray originates from either the actual
  // screen position (if available), or from an implementation-defined location
  // such as the near plane intersection.
  gfx.mojom.Transform? input_from_pointer;

  // List of 0 or more names describing both the preferred visual representation
  // and behavior of the associated input source. Each name is lowercase with no
  // spaces and has separate words concatenated with a "-". These names are
  // sorted in descending order of specificity.
  array<string> profiles;
};

// Sourced from https://immersive-web.github.io/webxr-hand-input/#idl-index
enum XRHandJoint {
  kWrist = 0,
  kThumbMetacarpal = 1,
  kThumbPhalanxProximal = 2,
  kThumbPhalanxDistal = 3,
  kThumbTip = 4,
  kIndexFingerMetacarpal = 5,
  kIndexFingerPhalanxProximal = 6,
  kIndexFingerPhalanxIntermediate = 7,
  kIndexFingerPhalanxDistal = 8,
  kIndexFingerTip = 9,
  kMiddleFingerMetacarpal = 10,
  kMiddleFingerPhalanxProximal = 11,
  kMiddleFingerPhalanxIntermediate = 12,
  kMiddleFingerPhalanxDistal = 13,
  kMiddleFingerTip = 14,
  kRingFingerMetacarpal = 15,
  kRingFingerPhalanxProximal = 16,
  kRingFingerPhalanxIntermediate = 17,
  kRingFingerPhalanxDistal = 18,
  kRingFingerTip = 19,
  kPinkyFingerMetacarpal = 20,
  kPinkyFingerPhalanxProximal = 21,
  kPinkyFingerPhalanxIntermediate = 22,
  kPinkyFingerPhalanxDistal = 23,
  kPinkyFingerTip = 24,
};

// This struct contains joint data and the associated joint for which the data
// corresponds.
// For each hand joint we are required to supply a transform and a radius.
struct XRHandJointData {
  // Which joint this data is for denoted by the XRHandJoint enum.
  XRHandJoint joint;
  // The transform of pose of this joint in mojo space.
  gfx.mojom.Transform? mojo_from_joint;
  // The radius of the joint in meters.
  float radius;
};

// The list of joint data. Must have data for each of the 25 joints.
struct XRHandTrackingData {
  // The transforms and radii of the joints
  array<XRHandJointData> hand_joint_data;
};

struct XRInputSourceState {
  // Uniquely identifies input source. The IDs are *not* guaranteed to be unique
  // within a session - they can potentially be reused by the device. The IDs
  // will be unique per frame data delivered, and the device must not reuse an
  // ID for at least one frame after the corresponding source is no longer
  // present.
  uint32 source_id;

  // Description of this input source's behavior. Should be mostly static, only
  // need periodic updates.
  XRInputSourceDescription? description;

  // Input device pose in mojo space. This is the grip pose for tracked
  // controllers, and the viewer pose for screen input.
  gfx.mojom.Transform? mojo_from_input;

  // True when position is estimated by something like a neck or arm model.
  // False when position is based on sensors tracking a 6DoF pose.
  bool emulated_position;

  // True if the input source should be considered auxiliary, as opposed to
  // primary. See https://immersive-web.github.io/webxr/#auxiliary-input-source.
  // Auxiliary input sources do not cause the select events to fire.
  // Note: some of subsequent fields are named "primary_*" - this does not imply
  // that the input source needs to have is_auxiliary set to false for those
  // fields to be present / valid. Input sources (even auxiliary) can have
  // multiple buttons - those fields are used to signify the state of those of
  // them that are considered primary for the specified input source.
  bool is_auxiliary;

  // Describes the current state of the primary input.
  bool primary_input_pressed;

  // Indicates if the input's primary input has been released (clicked) since
  // the last report. May indicate that the button was pressed and released in
  // the space of a single frame, so it may not have been preceeded by a
  // primary_input_pressed = true;
  bool primary_input_clicked;

  // Describes the current state of the primary squeeze.
  bool primary_squeeze_pressed;

  // Indicates if the input's primary sequeeze has been released (clicked) since
  // the last report. May indicate that the button was pressed and released in
  // the space of a single frame, so it may not have been preceeded by a
  // primary_squeeze_pressed = true;
  bool primary_squeeze_clicked;

  // Contains information about advanced input state such as additional buttons,
  // triggers, touch sensors, joysticks, touchpads, and vibration actuators. If
  // the controller has none of these capabilities, then this field will be
  // null. The information is a snapshot of the controller state that was taken
  // at the time corresponding to the timestamp field's value.
  Gamepad? gamepad;

  // If DOM Overlay is enabled and active, and if this input source's pointer
  // ray intersects the DOM overlay element, this contains the 2D screen space
  // coordinates of that intersection point within the DOM overlay content
  // element, with origin at the top left. Null if there is no intersection
  // point, or if DOM Overlay is not active or not supported. (Currently, only
  // the ARCore device's immersive-ar mode supports it when requested via
  // XRSessionFeature.DOM_OVERLAY aka "dom-overlay".)
  gfx.mojom.PointF? overlay_pointer_position;

  // If Hand-Tracking is enabled and active, and if this input source is
  // one which supports hand tracking, this contains the hand tracking data
  // for the input source
  XRHandTrackingData? hand_tracking_data;
};

//
// WebVR/WebXR shared interfaces
//

// A field of view, given by 4 degrees describing the view from a center point.
// For a typical field of view that contains the center point, all angles are
// positive.
struct VRFieldOfView {
  float up_degrees;
  float down_degrees;
  float left_degrees;
  float right_degrees;
};

// An entity's pose. Used by entities for which either position or orientation
// may sometimes not be known. Also supports those poses where the position may
// be emulated and this is worth telling consumers.
struct VRPose {
  gfx.mojom.Quaternion? orientation;
  gfx.mojom.Point3F? position;

  // True when position is estimated by something like a neck or arm model.
  // False when position is based on sensors tracking a 6DoF pose.
  bool emulated_position;
};

// An entity's pose. Used by entities for which both position and orientation is
// always known (e.g. anchors, planes).
struct Pose {
  gfx.mojom.Quaternion orientation;
  gfx.mojom.Point3F position;
};

struct XRRay {
  gfx.mojom.Point3F origin;
  gfx.mojom.Vector3dF direction;
};

struct XRHitResult {
  Pose mojo_from_result;  // Pose of the result (aka intersection point).
  uint64 plane_id;  // If the hit test result was computed based off of a plane,
                    // the plane ID will be stored here. 0 signifies no plane.
                    // TODO(https://crbug.com/657632) - make it optional once
                    // mojo supports optional numeric types.
};

// Describes the eye that a view is associated with. kNone represents views that
// are not associated with an eye, such as monoscopic displays or views exposed
// through secondary views.
enum XREye {
  kNone = 0,
  kLeft = 1,
  kRight = 2,
};

// Information about a view for a given XR frame. A frame contains the state of
// the XR device for a particular timestamp. A view represents the display or
// a portion of the display, and contains all the information required to render
// content to the view.
struct XRView {
  XREye eye;
  VRFieldOfView field_of_view;

  // The pose transform of this view relative to the device's native origin.
  // This transformation is equivalent to mojo_from_viewer * viewer_from_view.
  // OpenXR has an API that directly returns this transformation matrix. ARCore
  // and GVR only expose the head pose (aka. mojo_from_viewer) and the
  // head-from-eye pose (viewer_from_view), so this calculation needs to be
  // done in the XR process before sending to the renderer.
  // TODO(https://crbug.com/1070380): All of the runtimes currently assume mojo
  // and local space are equivalent, so this is the same as local_from_view.
  // Instead, send this in mojo space and send the relevant transforms to other
  // spaces in XRFrameData so the clients can calculate them as needed.
  gfx.mojom.Transform mojo_from_view;

  // A viewport describes the rectangular region that the underlying graphics
  // API should use to render this view.
  // See https://immersive-web.github.io/webxr/#xrviewport
  gfx.mojom.Rect viewport;

  // A first person observer view is a secondary view typically used when
  // capturing from the camera of an AR device.
  // https://immersive-web.github.io/webxr-ar-module/#first-person-observer-view
  bool is_first_person_observer = false;
};

struct VRStageParameters {
  gfx.mojom.Transform mojo_from_floor;

  // If present, indicates that the device supports a bounded reference space
  // (https://immersive-web.github.io/webxr/#xrboundedreferencespace-interface).
  // The points are expected to be in the "standing" space (i.e. there should be
  // no need to transform them by the accompanying standing transform) and
  // in a clockwise winding order.
  array<gfx.mojom.Point3F>? bounds;
};

// Frame transport method from the Renderer's point of view.
enum XRPresentationTransportMethod {
  NONE = 0,

  // Renderer should create a new texture handle (Windows) or
  // texture mailbox (Android Surface path) containing the
  // frame's image and supply that as a submitFrame argument.
  SUBMIT_AS_TEXTURE_HANDLE = 1,
  SUBMIT_AS_MAILBOX_HOLDER = 2,

  // Renderer should draw directly into a texture mailbox
  // provided for each frame in OnVSync.
  DRAW_INTO_TEXTURE_MAILBOX = 3,
};

struct XRPresentationTransportOptions {
  XRPresentationTransportMethod transport_method;

  // Booleans indicating which of the XRPresentationClient callbacks
  // are in use. Default is false, the device implementation should set
  // the ones to true that it needs and can ignore the rest.
  bool wait_for_transfer_notification;
  bool wait_for_render_notification;
  bool wait_for_gpu_fence;
};

// Native origins that are reference spaces are identified by their type.
// Used for metrics, don't remove or change values.
enum XRReferenceSpaceType {
  kViewer = 0,
  kLocal = 1,
  kLocalFloor = 2,
  kBoundedFloor = 3,
  kUnbounded = 4,
};

// Native origin represents a reference space that is known to the device and
// whose position can be tracked over time. There are multiple different types
// of native origins (planes, anchors, reference spaces, input sources), each of
// them roughly corresponds to something that either is an XRSpace (for example
// XRReferenceSpaceType, XRBoundedReferenceSpace) or returns an XRSpace (for
// example XRAnchor, XRPlane, XRInputSource). Native origin can be identified,
// depending on its type, by the id of the entity (this is the case for planes
// and anchors), index (for tracked images), by reference space type, handedness
// & joint, or by input source id & input source space type.
union XRNativeOriginInformation {
  XRInputSourceSpaceInfo input_source_space_info;
  uint64 plane_id;
  uint64 anchor_id;
  XRReferenceSpaceType reference_space_type;
  XRHandJointSpaceInfo hand_joint_space_info;
  uint32 image_index;
};

// Input sources have 2 kinds of native origins, each of them backs a different
// type of XRSpace. See https://immersive-web.github.io/webxr/#xrinputsource,
// specifically `targetRaySpace` & `gripSpace` members.
enum XRInputSourceSpaceType {
  kTargetRay = 0,
  kGrip = 1,
};

// To uniquely identify a native origin derived from an input source, we need to
// know the input source's id and the type of the space.
struct XRInputSourceSpaceInfo {
  uint32 input_source_id;
  XRInputSourceSpaceType input_source_space_type;
};

// Since there is no single way to identify a joint as it may be on the left or
// right hand, we include the handedness along with the joint in order to create
// a unique method of identification.
struct XRHandJointSpaceInfo {
  XRHandedness handedness;
  XRHandJoint joint;
};

enum XRPlaneOrientation {
  UNKNOWN = 0,
  HORIZONTAL = 1,
  VERTICAL = 2
};

struct XRPlanePointData {
  float x;
  float z;
};

// Struct containing plane-related information. A plane describes a flat surface
// detected in the real world by the underlying system.
struct XRPlaneData {
  // Unique (within a session) identifier of the plane.
  uint64 id;

  // Plane orientation relative to gravity.
  XRPlaneOrientation orientation;

  // Pose of the plane's center. Defines new coordinate space.
  // Y axis of the coordinate space describes plane's normal, the rotation of
  // X and Z around the Y axis is arbitrary.
  // Null if the device does not know where the plane is located in the world
  // space (tracking loss), but the tracking can still be recovered.
  Pose? mojo_from_plane;

  // Vertices of 2D convex polygon approximating the plane.
  array<XRPlanePointData> polygon;
};

// Struct containing information about all tracked & updated planes in a given
// frame. If a plane tracked in frame N-1 is no longer being tracked in frame N,
// its ID will not be present in all_plane_ids and its XRPlaneData will not be
// present in updated_planes_data.
struct XRPlaneDetectionData {
  // Array with ids of all tracked planes.
  array<uint64> all_planes_ids;

  // Array with plane data for all updated planes. Plane is considered updated
  // if its position or polygon has changed. Updated planes are always a subset
  // of all planes (i.e. for each plane found in updated_planes_data, its ID
  // will be present in all_planes_ids).
  array<XRPlaneData> updated_planes_data;
};

// Struct containing anchor-related information. An anchor represents a pose
// (position and orientation) in the real world. The anchor's pose will be
// adjusted by the underlying system as its understanding of the real world
// evolves.
struct XRAnchorData {
  // Unique (within a session) identifier of the anchor.
  uint64 id;

  // Pose of the anchor. Null if the device does not know where the anchor is
  // located in the world space (tracking loss), but the tracking can still be
  // recovered.
  Pose? mojo_from_anchor;
};

// Struct containing information about all tracked & updated anchors in a given
// frame. If an anchor tracked in frame N-1 is no longer being tracked in
// frame N, its ID will not be present in all_anchors_ids and its XRAnchorData
// will not be present in updated_anchors_data.
struct XRAnchorsData {
  // Array with ids of all tracked anchors.
  array<uint64> all_anchors_ids;

  // Array with anchor data for all updated anchors. Updated anchors are always
  // a subset of all anchors (i.e. for each anchor found in
  // updated_anchors_data, its ID will be present in all_anchors_ids).
  array<XRAnchorData> updated_anchors_data;
};

// Information about results for a single subscription for the current frame.
// This struct is relevant for subscriptions to hit test for persistent input
// sources.
struct XRHitTestSubscriptionResultData {
  uint64 subscription_id;
  array<XRHitResult> hit_test_results;
};

// Information about results for a single subscription for the current frame.
// This struct is relevant for subscriptions to hit test for transient input
// sources.
struct XRHitTestTransientInputSubscriptionResultData {
  uint64 subscription_id;
  // Map from input source id to array of hit test results obtained using that
  // input source. Required since there might be multiple input sources that
  // match input source profile name passed in at subscription creation (see
  // XRFrameDataProvider::SubscribeToHitTestForTransientInput()).
  map<uint32, array<XRHitResult>> input_source_id_to_hit_test_results;
};

// Struct containing data about results of all hit test subscriptions for the
// current frame.
struct XRHitTestSubscriptionResultsData {
  // Results for non-transient input sources.
  array<XRHitTestSubscriptionResultData> results;
  // Results for transient input sources.
  array<XRHitTestTransientInputSubscriptionResultData> transient_input_results;
};

// Tuple of single-precision floating point RGB data.
// This is the format used by ArCore for spherical harmonics coefficients
// and main light intensity.
struct RgbTupleF32 {
  float red;
  float green;
  float blue;
};

// Contains the coefficient data for L2 spherical harmonics generated by ArCore.
struct XRSphericalHarmonics {
  // L2 (3rd order) spherical harmonics coefficients.
  // This array has 9 RGB vector elements, representing the 27 coefficients
  // given by ArCore.
  // Units are expressed in nits:
  // https://github.com/immersive-web/lighting-estimation/blob/master/lighting-estimation-explainer.md#physically-based-units
  array<RgbTupleF32, 9> coefficients;
};

// Tuple of half-precision floating point RGBA data.
// Since there isn't a native way to represent half-precision floats, they are
// stored a uint16s.
// This is the pixel format used by ArCore for cubemap textures.
struct RgbaTupleF16 {
  uint16 red;
  uint16 green;
  uint16 blue;
  uint16 alpha;
};

// Contains the pixel data for all six faces of an HDR cubemap generated by
// ArCore. Pixel data is stored in RGBA16F format.
struct XRCubeMap {
  // Each pixel has four components (RGBA)
  const uint64 kNumComponentsPerPixel = 4;

  // The width and height (in pixels) of each cube map face.
  // Since each face is equal and square, this is only given as a single value.
  // Additionally, the dimensions of each face will be a power of two.
  // Note that each pixel is a contiguous RGBA16F vector, so the length of each
  // array will be this times |kNumComponentsPerPixel|.
  uint32 width_and_height;

  // Each cube map face, stored in RGBA row-major order. Units are in nits.
  // Each array will be |width_and_height ** 2| in length.
  // https://github.com/immersive-web/lighting-estimation/blob/master/lighting-estimation-explainer.md#physically-based-units
  array<RgbaTupleF16> positive_x;
  array<RgbaTupleF16> negative_x;
  array<RgbaTupleF16> positive_y;
  array<RgbaTupleF16> negative_y;
  array<RgbaTupleF16> positive_z;
  array<RgbaTupleF16> negative_z;
};

// Contains lighting information generated from ArCore for use in diffuse
// lighting calculations.
struct XRLightProbe {
  XRSphericalHarmonics spherical_harmonics;
  gfx.mojom.Vector3dF main_light_direction;

  // RGB vector, units in nits.
  RgbTupleF32 main_light_intensity;
};

// Contains lighting information generated from ArCore for use in specular
// lighting calculations.
struct XRReflectionProbe {
  XRCubeMap cube_map;
};

// Contains all lighting information generated from ArCore.
struct XRLightEstimationData {
  XRLightProbe? light_probe;
  XRReflectionProbe? reflection_probe;
};

// Structure that signifies that the depth data is still valid, no additional
// information is provided. See |XRDepthData| for more details.
struct XRDepthDataStillValid {};

// Structure that signifies that the Depth data was updated. Provides
// information about current depth data. See |XRDepthData| for more details.
struct XRDepthDataUpdated {
  // Timestamp of the returned data, in unspecified base.
  mojo_base.mojom.TimeDelta time_delta;
  // Array of 16-bit numbers representing depth in millimeters from the camera
  // plane.
  mojo_base.mojom.BigBuffer pixel_data;
  // Transform that needs to be applied when indexing into pixel_data when using
  // normalized view coordinates (with origin at bottom left corner of the
  // screen).
  gfx.mojom.Transform norm_texture_from_norm_view;
  // Size of the pixel array. Valid iff pixel_data is not null.
  gfx.mojom.Size size;
  // Factor that needs to be applied when converting from raw values in the
  // |pixel_data| buffer into meters.
  float raw_value_to_meters;
};

// Depth data may be the same as the one returned in the previous frame - it is
// represented as a union of (empty) XRDepthDataStillValid structure that
// conveys no additional information except that the previously returned depth
// data is still valid, and the XRDepthDataUpdated structure that signifies that
// the depth data was updated & carries all information about it.
union XRDepthData {
  XRDepthDataStillValid data_still_valid;
  XRDepthDataUpdated updated_depth_data;
};

struct XRTrackedImageData {
  // Index of the image within the tracked images array passed to requestSession
  uint32 index;

  // Pose of the image. Must be non-null, images are only included in results
  // if they have a pose.
  Pose mojo_from_image;

  // If true, the image is currently actively tracked. If false, the pose is
  // inferred from previous tracking results. In that case, it's likely
  // accurate for stationary images, but may be stale for a moving image.
  bool actively_tracked;

  // The best-effort measured width in meters for the physical image. Zero
  // if the width is unknown due to the image never having been tracked
  // successfully.
  float width_in_meters;
};

struct XRTrackedImagesData {
  array<XRTrackedImageData> images_data;

  // The first returned result includes an array of trackable/untrackable
  // scores, in the same order as tracked_images in the session request.
  // (This is unambiguous since only used for immersive sessions, and
  // there can only be one immersive session at a time.)
  array<bool>? image_trackable_scores;
};

// The data needed for each animation frame of an XRSession.
struct XRFrameData {
  // General XRSession value

  // The viewer pose of this frame. The pose may be null if the device lost
  // tracking. The XRFrameData can still have other data, such as pass through
  // camera image.
  VRPose? mojo_from_viewer;
  // Time delta from an unspecified origin.
  mojo_base.mojom.TimeDelta time_delta;
  // The buffer_holder is used for sending imagery data back and forth across
  // the process boundary.
  gpu.mojom.MailboxHolder? buffer_holder;

  // The camera_image_buffer_holder is used to send a copy of the camera image
  // across the process boundary for immersive-ar sessions. This assumes that
  // there exists a single camera.
  gpu.mojom.MailboxHolder? camera_image_buffer_holder;
  gfx.mojom.Size? camera_image_size;

  // Depth data (if the device supports it and the environment integration is
  // enabled). This assumes that depth information is provided from a single
  // sensor. If for any reason the latest depth data could not be obtained, it
  // will be set to null.
  XRDepthData? depth_data;

  // Indicates that there has been a significant discontinuity in the mojo space
  // coordinate system, and that poses from this point forward with the same
  // coordinates as those received previously may not actually be in the same
  // position. This could have been triggered either via device specific UI or
  // due to a major change in tracking locatibility, but should not typically
  // be used when recovering from tracking loss.
  // This event should be bubbled up as a reference space reset event:
  // https://immersive-web.github.io/webxr/#eventdef-xrreferencespace-reset
  bool mojo_space_reset;

  // Exclusive session values

  // The frame_id maps frame data to a frame arriving from the compositor. IDs
  // will be reused after the frame arrives from the compositor. Negative IDs
  // imply no mapping.
  int16 frame_id;

  // Information about all the views in a frame. Views can appear in any order
  // so it should not be assumed the left eye comes before the right eye. It is
  // valid for the array to contain multiple views of the same eye. An example
  // of this scenario are displays that have dominant left/right eyes and inset
  // left/right eyes. In these scenarios, the dominant views comes before the
  // inset views in the array.
  array<XRView> views;

  // For immersive sessions only, reports the state of all active input devices
  // at the time of this frame. all controller state, including current
  array<XRInputSourceState>? input_state;

  // Stage parameters may be provided per-frame, or only re-computed
  // periodically.  However, setting the stage parameters to null is perfectly
  // valid in some cases (e.g. we've lost tracking), so we can't just use
  // whether they are present or not to indicate that they have been updated
  // so we compare IDs against the value sent in the XRFrameDataRequestOptions
  // to determine when updates are needed.
  uint32 stage_parameters_id;
  VRStageParameters? stage_parameters;

  // Detected plane information. Only present if plane detection is enabled.
  XRPlaneDetectionData? detected_planes_data;

  // Tracked anchors information. Only present if anchors are enabled.
  XRAnchorsData? anchors_data;

  // Lighting estimation data. Only present if lighting estimation is enabled.
  XRLightEstimationData? light_estimation_data;

  // Hit test subscription results. Only present if the session supports
  // environment integration.
  XRHitTestSubscriptionResultsData? hit_test_subscription_results;

  // If nonzero, an estimate of how much of the available render time budget
  // was used for GPU rendering for the most recent measured frame. A value
  // above 1.0 means that the application is dropping frames due to GPU load,
  // and a value well below 1.0 means that GPU utilization is low. This is
  // intended to be used as input for renderer-side adaptive viewport sizing.
  // A value of zero means the ratio is unknown and must not be used.
  float rendering_time_ratio;

  XRTrackedImagesData? tracked_images;
};

struct RequestSessionSuccess {
  XRSession session;

  // Used to report metrics during the session to the browser process.
  // See the note on |XRSessionMetricsRecorder| for context.
  pending_remote<XRSessionMetricsRecorder> metrics_recorder;
};

union RequestSessionResult {
  RequestSessionSuccess success;
  RequestSessionError failure_reason;
};

// Return value for VRService.MakeXrCompatible() to indicate whether the GPU
// process is compatible with the active VR headset.
enum XrCompatibleResult {
  // Compatibility results where the GPU process was not restarted.
  kAlreadyCompatible,         // GPU process is already on a compatible GPU.
  kNoDeviceAvailable,         // No WebXR device is available.
  kWebXrFeaturePolicyBlocked, // WebXR is blocked by feature policy.

  // Compatibility results where the GPU process was restarted. Context lost and
  // restored for existing WebGL contexts must be handled before using for XR.
  kCompatibleAfterRestart,    // XR compatible, GPU process was restarted.
  kNotCompatibleAfterRestart, // Not XR compatible, GPU process was restarted.
};

// Interface for requesting XRDevice interfaces and registering for
// notifications that the XRDevice has changed. Implemented in the browser
// process and consumed in the renderer process.
interface VRService {
  // Optionally supply a VRServiceClient to listen for changes to the XRDevice.
  // A bad message will be reported if this is called multiple times.
  SetClient(pending_remote<VRServiceClient> client);

  // Request to initialize a session in the browser process. If successful, the
  // XRSession struct with the requisite interfaces will be returned.
  RequestSession(XRSessionOptions options) => (RequestSessionResult result);
  SupportsSession(XRSessionOptions options) => (bool supports_session);

  // Shuts down an active immersive session. The callback is triggered
  // once teardown is complete and the system is again in a state where
  // a new immersive session could be started.
  ExitPresent() => ();

  // Used for the renderer process to indicate that it is throttling frame
  // requests from the page, and thus it (not the page) is responsible for any
  // drop in frame rate or delay in posting frames. This is mainly meant to be
  // informational for the browser process, so that it can decide if or what UI
  // to show if it seems like the frame rate is too low or has stalled. The
  // renderer can (and may) still decide to submit frames and that should not be
  // treated as illegal or clear the throttled state.
  SetFramesThrottled(bool throttled);

  // Request for the GPU process to be compatible with the active VR headset.
  // This will trigger the initialization of the XR process if it isn't already
  // initialized and then restart the GPU process if it's not using the same GPU
  // as the VR headset. The Sync attribute is needed because there are two ways
  // for WebXR to trigger this - WebGLRenderingContext.makeXRCompatible()
  // which returns a promise and is asynchronous, and
  // HTMLCanvasElement.getContext('webgl', { xrCompatible: true }) which is
  // synchronous and must return a context that is already xr compatible.
  [Sync]
  MakeXrCompatible() => (XrCompatibleResult xr_compatible_result);
};

// Any metrics that must be recorded throughout the duration of an XR session
// should be reported through this interface, so that the UKM event may still
// be recorded even if the session is shut down via user navigation.
interface XRSessionMetricsRecorder {
  // Records a use (or attempted use - the feature request
  // may not have been granted) of a feature.
  ReportFeatureUsed(XRSessionFeature feature);
};

// The interface for the renderer to listen to top level XR events, events that
// can be listened to and triggered without the renderer calling requestDevice.
interface VRServiceClient {
  // Signals changes to the available physical device runtimes.
  OnDeviceChanged();
};

enum CreateAnchorResult {
  SUCCESS = 0,
  FAILURE = 1,
};

enum SubscribeToHitTestResult {
  SUCCESS = 0,
  FAILURE_GENERIC = 1,
};

enum EntityTypeForHitTest {
  POINT = 1,
  PLANE = 2,
};

// Provides functionality for integrating environment information into an
// XRSession. For example, some AR sessions would implement hit test to allow
// developers to get the information about the world that its sensors supply.
// On XRSession end, the message pipe will be destroyed - we need to clean up
// all outstanding JavaScript promises at that time. The XRsession might end for
// example due to a call to XRSession::end() method (exposed to JavaScript).
interface XREnvironmentIntegrationProvider {
  // Establishes a hit test subscription on the device. The subscription results
  // will be computed taking into account latest state of the native origin
  // identified by passed in |native_origin_information|. I.e., in each frame,
  // the ray used to subscribe to hit test will be transformed to device
  // coordinate system using the device's current knowledge of the state of the
  // native origin. |entity_types| are used to filter out results based on the
  // specific type of the entity that was used to compute the hit test result -
  // only the results based on the entities passed in this array will be
  // returned in XRFrameData for this particular subscription.
  // The returned |subscription_id| uniquely identifies the subscription within
  // an immersive session. Lifetime of the subscription is tied to the lifetime
  // of the immersive session.
  // Hit-test-related mojo interfaces are influenced by WebXR's Hit Test API
  // proposal, details can be found here:
  // https://github.com/immersive-web/hit-test
  SubscribeToHitTest(
    XRNativeOriginInformation native_origin_information,
    array<EntityTypeForHitTest> entity_types,
    XRRay ray) => (SubscribeToHitTestResult result, uint64 subscription_id);

  // Establishes a hit test subscription on the device. The subscription results
  // will be computed taking into account latest state of the input sources that
  // are matching profile name specified in |profile_name|. I.e., in each frame,
  // the ray used to subscribe to hit test will be transformed to device
  // coordinate system using the device's current knowledge of the state of the
  // input sources. |entity_types| are used to filter out results based on the
  // specific type of the entity that was used to compute the hit test result -
  // only the results based on the entities passed in this array will be
  // returned in XRFrameData for this particular subscription.
  // The returned |subscription_id| uniquely identifies the subscription within
  // an immersive session. Lifetime of the subscription is tied to the lifetime
  // of the immersive session.
  // Hit-test-related mojo interfaces are influenced by WebXR's Hit Test API
  // proposal, details can be found here:
  // https://github.com/immersive-web/hit-test
  // Transient input is described here:
  // https://immersive-web.github.io/webxr/#transient-input
  SubscribeToHitTestForTransientInput(
    string profile_name,
    array<EntityTypeForHitTest> entity_types,
    XRRay ray) => (SubscribeToHitTestResult result, uint64 subscription_id);

  // Notifies the device that subscription with the passed in |subscription_id|
  // is no longer needed and should be removed. The |subscription_id| must be a
  // valid id of a subscription returned by one of the SubscribeToHitTest/
  // SubscribeToHitTestForTransientInput calls, otherwise the call will be
  // ignored.
  UnsubscribeFromHitTest(uint64 subscription_id);

  // Issues a request to create a free-floating anchor (not attached to any
  // particular real world entity).
  // |native_origin_information| specifies native origin relative to which the
  // anchor is supposed to be created. |native_origin_from_anchor| describes the
  // desired pose of the newly created anchor.
  // |result| will contain status code of the request. |anchor_id| will be valid
  // only if the |result| is SUCCESS.
  CreateAnchor(
    XRNativeOriginInformation native_origin_information,
    Pose native_origin_from_anchor)
      => (CreateAnchorResult result, uint64 anchor_id);
  // TODO(https://crbug.com/657632): Switch anchor_id to nullable integer once
  // that's available. This will allow us to remove CreateAnchorResult if we're
  // not interested in obtaining detailed error information from the device.

  // Issues a request to create an anchor attached to a plane.
  // |native_origin_information| specifies native origin relative to which the
  // anchor is supposed to be created. |native_origin_from_anchor| describes the
  // desired pose of the newly created anchor. |plane_id| identifies which plane
  // the anchor will be attached to.
  // |result| will contain status code of the request. |anchor_id| will be valid
  // only if the |result| is SUCCESS.
  CreatePlaneAnchor(
    XRNativeOriginInformation native_origin_information,
    Pose native_origin_from_anchor,
    uint64 plane_id)
      => (CreateAnchorResult result, uint64 anchor_id);
  // TODO(https://crbug.com/657632): Ditto - make anchor_id a nullable integer.

  // Detaches an existing anchor. The |anchor_id| must be a valid id of an
  // anchor created by one of the CreateAnchor calls, otherwise the call will be
  // ignored.
  DetachAnchor(uint64 anchor_id);
};

struct XRFrameDataRequestOptions {
  // Controls whether |XRFrameData.light_estimation_data| should be populated
  // by the request to |XRFrameDataProvider.GetFrameData()|.
  bool include_lighting_estimation_data;

  // The ID of the last known stage parameters set sent to the requesting
  // session. The XRFrameData should only send new stage parameters if it has
  // a higher ID. (Zero would indicate that the stage parameters should always
  // sent.)
  uint32 stage_parameters_id;
};

// Provides the necessary functionality for a WebXR session to get data for
// drawing frames. The kind of data it gets depends on what kind of session was
// requested.
// This interface is hosted in the Browser process, but will move to a sandboxed
// utility process on Windows.  The render process communicates with it.
interface XRFrameDataProvider {
  // frame_data is optional and will not be set if and only if the call fails
  // for some reason, such as device disconnection.
  GetFrameData(XRFrameDataRequestOptions? options) => (XRFrameData? frame_data);
  GetEnvironmentIntegrationProvider(
      pending_associated_receiver<XREnvironmentIntegrationProvider>
          environment_provider);
};

// Provides the necessary functionality for sending frames to a headset.
// This interface is hosted in the Browser process, but will move to a sandboxed
// utility process on Windows.  The render process communicates with it.
interface XRPresentationProvider {
  // This function tells the device which parts of the canvas should be rendered
  // to which view.
  UpdateLayerBounds(int16 frame_id, gfx.mojom.RectF left_bounds,
                    gfx.mojom.RectF right_bounds, gfx.mojom.Size source_size);

  // Call this if the animation loop exited without submitting a frame to
  // ensure that every GetFrameData has a matching Submit call. This happens for
  // WebXR if there were no drawing operations to the opaque framebuffer, and
  // for WebVR 1.1 if the application didn't call SubmitFrame. Usable with any
  // XRPresentationTransportMethod. This path does *not* call the
  // SubmitFrameClient methods such as OnSubmitFrameTransferred. This is
  // intended to help separate frames while presenting, it may or may not
  // be called for the last animating frame when presentation ends.
  SubmitFrameMissing(int16 frame_id, gpu.mojom.SyncToken sync_token);

  // XRPresentationTransportMethod SUBMIT_AS_MAILBOX_HOLDER
  SubmitFrame(int16 frame_id, gpu.mojom.MailboxHolder mailbox_holder,
              mojo_base.mojom.TimeDelta time_waited);

  // XRPresentationTransportMethod SUBMIT_AS_TEXTURE_HANDLE
  // |texture| should only be used after |sync_token| has passed.
  [EnableIf=is_win]
  SubmitFrameWithTextureHandle(int16 frameId, handle<platform> texture,
                               gpu.mojom.SyncToken sync_token);

  // XRPresentationTransportMethod DRAW_INTO_TEXTURE_MAILBOX
  SubmitFrameDrawnIntoTexture(int16 frameId, gpu.mojom.SyncToken sync_token,
                              mojo_base.mojom.TimeDelta time_waited);
};

// After submitting a frame, the XRPresentationProvider will notify the client
// about several stages of the render pipeline.  This allows pipelining
// efficiency.  Following XRPresentationProvider::Submit*, the submitted frame
// will be transferred (read from, perhaps copied to another texture), and then
// rendered (submitted to the underlying VR API).
// The client lives in the render process.
//
// See XRPresentationTransportOptions which configures which of these
// callbacks are in use.
interface XRPresentationClient {
  // The XRPresentationProvider calls this to indicate that the submitted frame
  // has been transferred, so the backing data (mailbox or GpuMemoryBuffer) can
  // be reused or discarded.  Note that this is a convenience/optimization
  // feature, not a security feature - if a site discards the data early we may
  // drop a frame, but nothing will otherwise misbehave.
  // When the frame wasn't successfully transferred, the client should create a
  // new mailbox/GpuMemoryBuffer rather than reusing an existing one to recover
  // for subsequent frames.
  OnSubmitFrameTransferred(bool success);

  // The XRPresentationProvider calls this after the frame was handed off to the
  // underlying VR API. This allows some pipelining of CPU/GPU work, while
  // delaying expensive tasks for a subsequent frame until the current frame has
  // completed.
  OnSubmitFrameRendered();

  // This callback provides a GpuFence corresponding to the previous frame's
  // rendering completion, intended for use with a server wait issued before
  // the following wait to prevent its rendering work from competing with
  // the previous frame.
  OnSubmitFrameGpuFence(gfx.mojom.GpuFenceHandle gpu_fence_handle);
};

enum XRVisibilityState {
  // Indicates the WebXR content is visible to the user and receiving input.
  VISIBLE = 1,
  // Indicates the WebXR content is visible but not receiving input.
  VISIBLE_BLURRED = 2,
  // Indicates the WebXR content is not visible.
  HIDDEN = 3
};

// Functions for pushing device information to the sessions.
interface XRSessionClient {
  OnExitPresent();
  // Indicates a change in the visibility of the WebXR content.
  OnVisibilityStateChanged(XRVisibilityState visibility_state);
};
