<html>
<head>
  <script type="text/javascript" src="webrtc_test_utilities.js"></script>
  <script type="text/javascript" src="webrtc_test_common.js"></script>
  <script type="text/javascript" src="webrtc_test_audio.js"></script>
  <script type="text/javascript">
  $ = function(id) {
    return document.getElementById(id);
  };

  let localPeerConnection = null;
  let remotePeerConnection = null;
  let remoteAudioTrack = null;
  let localAudioTrack = null;
  let remoteVideoTrack = null;
  let localVideoTrack = null;

  // The second set of constraints should request audio (e.g. audio:true) since
  // we expect audio to be playing after the second renegotiation.
  async function callAndRenegotiateToAudio(constraints, renegotiationConstraints) {
    createConnections(null);
    try {
      // Get a new stream with renegotiation constraints and add it to
      // the first connection.
      const stream = await createAudioContextSound(renegotiationConstraints);
      await addStreamToTheFirstConnectionAndNegotiate(stream);

      // Wait for the first connection to stabilize again.
      await waitForConnectionToStabilize(localPeerConnection);

      // Ensure audio is playing on the second connection.
      await ensureAudioPlaying(remoteAudioTrack);

      // Log success
      logSuccess();
    } catch (error) {
      console.error("An error occurred:", error);
      assertTrue(false);
    }
  }

  function callAndEnsureAudioIsPlaying(constraints) {
    return setupCallAndPromiseAudioPlaying(constraints)
        .then(logSuccess);
  }

  function callAndEnsureRemoteAudioTrackMutingWorks(constraints) {
    return setupCallAndPromiseAudioPlaying(constraints).then(() => {
      // Call is up, now mute the remote track and check we stop playing out
      // audio (after a small delay, we don't expect it to happen instantly).
      enableRemoteAudio(false);

      return new Promise(resolve => {
        setTimeout(resolve, 250);
      }).then(() => {
        // Ensure that ensureSilence result is returned.
        return ensureSilence(remoteAudioTrack);
      });
    }).then(logSuccess());
  }

  function callAndEnsureLocalAudioTrackMutingWorks(constraints) {
    return setupCallAndPromiseAudioPlaying(constraints).then(() => {
      // Call is up, now mute the local track of the sending side and ensure
      // the receiving side stops receiving audio.
      enableLocalAudio(false);

      return new Promise(resolve => {
        setTimeout(resolve, 250);
      })
      .then(() => {
        return ensureSilence(remoteAudioTrack)
      });
    })
    .then(logSuccess);
  }

  function callAndEnsureAudioTrackUnmutingWorks(constraints) {
    return setupCallAndPromiseAudioPlaying(constraints).then(() => {
      // Mute, wait a while, unmute, verify audio gets back up.
      // (Also, ensure video muting doesn't affect audio).
      enableRemoteAudio(false);
      enableRemoteVideo(false);

      setTimeout(function() {
        enableRemoteAudio(true);
      }, 500);

      return new Promise(resolve => {
        setTimeout(resolve, 1500);
      })
      .then(() => ensureAudioPlaying(remoteAudioTrack));
    })
    .then(logSuccess);
  }

  function callAndEnsureMuteWorksForMediaRecorder(constraints) {
    return setupCallAndPromiseAudioPlaying(constraints).then(() => {
      // Mute and ensure the MediaRecorder has silent data.
      enableRemoteAudio(false);

      let blobParts = [];
      const mediaRecorder = setupMediaRecorder();
      mediaRecorder.ondataavailable =function(event) {
        console.log('ondataavailable size:' + event.data.size);
        blobParts.push(event.data);
      };

      mediaRecorder.start();
      const onStopPromise = new Promise(resolve => {
        mediaRecorder.onstop = function() {
          console.log('onstop');
          assertTrue(blobParts.length > 0, 'Blob should have data');
          const blob = new Blob(blobParts, { type: 'audio/ogg; codecs=opus' });
          isSilentAudioBlob(blob).then(isSilent => {
            assertTrue(isSilent, 'Blob should be silent');
            resolve(); // finish the promise chain
          });
        };
      });

      return new Promise(resolve => setTimeout(resolve, 1000))
        .then(() => {
          mediaRecorder.stop();
          return onStopPromise; // wait for MediaRecorder to finish
        });
    })
    .then(logSuccess);
  }

  function callAndEnsureLocalVideoMutingDoesntMuteAudio(constraints) {
    return setupCallAndPromiseAudioPlaying(constraints).then(() => {
      enableLocalVideo(false);
      return ensureAudioPlaying(remoteAudioTrack)
        .then(logSuccess);
    });
  }

  function callAndEnsureRemoteVideoMutingDoesntMuteAudio(constraints) {
    return setupCallAndPromiseAudioPlaying(constraints).then(() => {
      enableRemoteVideo(false);
      return ensureAudioPlaying(remoteAudioTrack)
          .then(logSuccess);
    });
  }

  // TODO(crbug.com/40637961): This test is a temporary replacement for:
  // external/wpt/webrtc/RTCRtpReceiver-getSynchronizationSources.https.html
  async function testEstablishAudioOnlyCallAndVerifyGetSynchronizationSourcesWorks() {
    const startTime = performance.timeOrigin + performance.now();

    await setupCallAndPromiseAudioPlaying({audio: true});

    const peerConnection = remotePeerConnection;

    const receivers = peerConnection.getReceivers();
    assertEquals(receivers.length, 1);
    const receiver = receivers[0];
    assertEquals(receiver.track.kind, 'audio');

    const results = receiver.getSynchronizationSources();
    assertEquals(results.length, 1);
    const result = results[0];

    const endTime = performance.timeOrigin + performance.now();

    console.log('getSynchronizationSources() = ' + JSON.stringify(result));

    // timestamp
    assertEquals(typeof result.timestamp, 'number');
    assertTrue(result.timestamp >= startTime);
    assertTrue(result.timestamp <= endTime);

    // source
    assertEquals(typeof result.source, 'number');
    assertTrue(result.source >= 0);
    assertTrue(result.source <= 0xffffffff);

    // rtpTimestamp
    assertEquals(typeof result.rtpTimestamp, 'number');
    assertTrue(result.rtpTimestamp >= 0);
    assertTrue(result.rtpTimestamp <= 0xffffffff);

    // audioLevel
    assertEquals(typeof result.audioLevel, 'number');
    assertTrue(result.audioLevel >= 0);
    assertTrue(result.audioLevel <= 1);

    // voiceActivityFlag
    if (result.voiceActivityFlag != undefined) {
      assertEquals(typeof result.voiceActivityFlag, 'boolean');
    }

    return logSuccess();
  }

  function createConnections(constraints) {
    localPeerConnection = createConnection(constraints, 'remote-view-1');
    assertEquals('stable', localPeerConnection.signalingState);

    remotePeerConnection = createConnection(constraints, 'remote-view-2');
    assertEquals('stable', remotePeerConnection.signalingState);
  }

  function createConnection(constraints, remoteView) {
    var pc = new RTCPeerConnection(null, constraints);
    pc.onaddstream = function(event) {
      onRemoteStream(event, remoteView);
    }
    return pc;
  }

  function displayAndRemember(localStream) {
    $('local-view').srcObject = localStream;
  }


  // Creates MediaStream with videoTrack from the getUserMedia but audioTrack
  // from the AudioContext oscillator because audio data from the getUserMedia
  // is unreliable.
  async function createAudioContextSound(constraints) {
    // Create an AudioContext
    const audioContext = new AudioContext;

    // Create an oscillator
    const oscillator = audioContext.createOscillator();
    oscillator.type = 'sine';
    oscillator.frequency.setValueAtTime(440, audioContext.currentTime);

    // Create a MediaStreamAudioDestinationNode.
    const destination = audioContext.createMediaStreamDestination();

    // Connect the oscillator to the destination.
    oscillator.connect(destination);

    // Start the oscillator
    oscillator.start();

    // The destination.stream contains the audio data as a MediaStream
    const mediaStreamFromAudioContext = destination.stream;

    // Combine the video track from getUserMedia with the audio track from the
    // AudioContext.
    const stream = await navigator.mediaDevices.getUserMedia(constraints);
    stream.getAudioTracks().forEach((track) => {
      stream.removeTrack(track);
    });
    stream.addTrack(mediaStreamFromAudioContext.getAudioTracks()[0]);
    return stream;
  }

  async function setupCallAndPromiseAudioPlaying(constraints) {
    try {
      createConnections(null);

      // Add the local stream to gFirstConnection to play one-way audio
      const stream = await createAudioContextSound(constraints);
      await addStreamToTheFirstConnectionAndNegotiate(stream);

      // Wait for the first connection to stabilize
      await waitForConnectionToStabilize(localPeerConnection);

      // Ensure audio is playing on the second connection
      await ensureAudioPlaying(remoteAudioTrack);
    } catch (error) {
      console.error("An error occurred during setup:", error);
      assertTrue(false);
    }
  }

  // Called if getUserMedia succeeds when we want to send from one connection.
  async function addStreamToTheFirstConnectionAndNegotiate(localStream) {
    displayAndRemember(localStream);
    let expectedTrackCount = 0;
    localStream.getTracks().forEach((track) => {
      localPeerConnection.addTrack(track, localStream);
      if (track.kind === "audio") {
        console.log("Add audio track");
        localAudioTrack = track; // Save to global audio track variable.
        expectedTrackCount++;
      } else if (track.kind === "video") {
        console.log("Add video track:");
        localVideoTrack = track; // Save to global video track variable.
        expectedTrackCount++;
      }
    });
    return negotiate(expectedTrackCount);
  }

  async function negotiate(expectedTrackCount) {
    let trackCount = 0;
    return new Promise(resolve => {
      remotePeerConnection.ontrack = event => {
        trackCount++;
        const track = event.track;
        if (track.kind === "audio") {
          console.log("Audio track received:");
          remoteAudioTrack = track; // Save to global audio track variable
        } else if (track.kind === "video") {
          console.log("Video track received:");
          remoteVideoTrack = track; // Save to global video track variable
        }

        if (trackCount === expectedTrackCount) {
          resolve();
        }
      };
      negotiateBetween(localPeerConnection, remotePeerConnection);
    });
  }

  function onRemoteStream(e, target) {
    console.log("Receiving remote stream...");
    var remoteVideo = $(target);
    remoteVideo.srcObject = e.stream;
  }

  function enableRemoteVideo(enabled) {
    remoteVideoTrack.enabled = enabled;
  }

  function enableRemoteAudio(enabled) {
    remoteAudioTrack.enabled = enabled;
  }

  function enableLocalVideo(enabled) {
    enableLocalVideo.enabled = enabled;
  }

  function enableLocalAudio(enabled) {
    localAudioTrack.enabled = enabled;
  }

  function setupMediaRecorder() {
    const stream = new MediaStream();
    stream.addTrack(remoteAudioTrack);
    const mediaRecorder = new MediaRecorder(stream);
    return mediaRecorder;
  }

  </script>
</head>
<body>
  <table border="0">
    <tr>
      <td><video width="320" height="240" id="local-view" style="display:none"
          autoplay muted></video></td>
      <td><video width="320" height="240" id="remote-view-1"
          style="display:none" autoplay></video></td>
      <td><video width="320" height="240" id="remote-view-2"
          style="display:none" autoplay></video></td>
      <!-- Canvases are named after their corresponding video elements. -->
      <td><canvas width="320" height="240" id="remote-view-1-canvas"
          style="display:none"></canvas></td>
      <td><canvas width="320" height="240" id="remote-view-2-canvas"
          style="display:none"></canvas></td>
    </tr>
  </table>
</body>
</html>
