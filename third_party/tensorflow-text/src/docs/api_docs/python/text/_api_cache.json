{
  "duplicate_of": {
    "text.BertTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.BertTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.BertTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.BertTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.BertTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.BertTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.BertTokenizer.split": "text.Tokenizer.split",
    "text.BertTokenizer.split_with_offsets": "text.TokenizerWithOffsets.split_with_offsets",
    "text.Detokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.Detokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.Detokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.Detokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.Detokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.Detokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.Detokenizer.__new__": "text.BertTokenizer.__new__",
    "text.FirstNItemSelector.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.FirstNItemSelector.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.FirstNItemSelector.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.FirstNItemSelector.__le__": "text.keras.layers.ToDense.__le__",
    "text.FirstNItemSelector.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.FirstNItemSelector.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.FirstNItemSelector.__new__": "text.BertTokenizer.__new__",
    "text.HubModuleSplitter.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.HubModuleSplitter.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.HubModuleSplitter.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.HubModuleSplitter.__le__": "text.keras.layers.ToDense.__le__",
    "text.HubModuleSplitter.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.HubModuleSplitter.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.HubModuleSplitter.__new__": "text.BertTokenizer.__new__",
    "text.HubModuleTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.HubModuleTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.HubModuleTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.HubModuleTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.HubModuleTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.HubModuleTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.HubModuleTokenizer.__new__": "text.BertTokenizer.__new__",
    "text.HubModuleTokenizer.split": "text.Tokenizer.split",
    "text.HubModuleTokenizer.split_with_offsets": "text.TokenizerWithOffsets.split_with_offsets",
    "text.MaskValuesChooser.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.MaskValuesChooser.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.MaskValuesChooser.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.MaskValuesChooser.__le__": "text.keras.layers.ToDense.__le__",
    "text.MaskValuesChooser.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.MaskValuesChooser.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.MaskValuesChooser.__new__": "text.BertTokenizer.__new__",
    "text.RandomItemSelector.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.RandomItemSelector.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.RandomItemSelector.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.RandomItemSelector.__le__": "text.keras.layers.ToDense.__le__",
    "text.RandomItemSelector.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.RandomItemSelector.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.RandomItemSelector.__new__": "text.BertTokenizer.__new__",
    "text.RandomItemSelector.unselectable_ids": "text.FirstNItemSelector.unselectable_ids",
    "text.RegexSplitter.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.RegexSplitter.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.RegexSplitter.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.RegexSplitter.__le__": "text.keras.layers.ToDense.__le__",
    "text.RegexSplitter.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.RegexSplitter.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.RegexSplitter.__new__": "text.BertTokenizer.__new__",
    "text.RoundRobinTrimmer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.RoundRobinTrimmer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.RoundRobinTrimmer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.RoundRobinTrimmer.__le__": "text.keras.layers.ToDense.__le__",
    "text.RoundRobinTrimmer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.RoundRobinTrimmer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.RoundRobinTrimmer.__new__": "text.BertTokenizer.__new__",
    "text.SentencepieceTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.SentencepieceTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.SentencepieceTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.SentencepieceTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.SentencepieceTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.SentencepieceTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.SentencepieceTokenizer.__new__": "text.BertTokenizer.__new__",
    "text.SentencepieceTokenizer.split": "text.Tokenizer.split",
    "text.SentencepieceTokenizer.split_with_offsets": "text.TokenizerWithOffsets.split_with_offsets",
    "text.SplitMergeFromLogitsTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.SplitMergeFromLogitsTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.SplitMergeFromLogitsTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.SplitMergeFromLogitsTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.SplitMergeFromLogitsTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.SplitMergeFromLogitsTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.SplitMergeFromLogitsTokenizer.__new__": "text.BertTokenizer.__new__",
    "text.SplitMergeFromLogitsTokenizer.split": "text.Tokenizer.split",
    "text.SplitMergeFromLogitsTokenizer.split_with_offsets": "text.TokenizerWithOffsets.split_with_offsets",
    "text.SplitMergeTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.SplitMergeTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.SplitMergeTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.SplitMergeTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.SplitMergeTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.SplitMergeTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.SplitMergeTokenizer.__new__": "text.BertTokenizer.__new__",
    "text.SplitMergeTokenizer.split": "text.Tokenizer.split",
    "text.SplitMergeTokenizer.split_with_offsets": "text.TokenizerWithOffsets.split_with_offsets",
    "text.Splitter.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.Splitter.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.Splitter.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.Splitter.__init__": "text.Detokenizer.__init__",
    "text.Splitter.__le__": "text.keras.layers.ToDense.__le__",
    "text.Splitter.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.Splitter.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.Splitter.__new__": "text.BertTokenizer.__new__",
    "text.SplitterWithOffsets.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.SplitterWithOffsets.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.SplitterWithOffsets.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.SplitterWithOffsets.__init__": "text.Detokenizer.__init__",
    "text.SplitterWithOffsets.__le__": "text.keras.layers.ToDense.__le__",
    "text.SplitterWithOffsets.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.SplitterWithOffsets.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.SplitterWithOffsets.__new__": "text.BertTokenizer.__new__",
    "text.SplitterWithOffsets.split": "text.Splitter.split",
    "text.StateBasedSentenceBreaker.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.StateBasedSentenceBreaker.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.StateBasedSentenceBreaker.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.StateBasedSentenceBreaker.__le__": "text.keras.layers.ToDense.__le__",
    "text.StateBasedSentenceBreaker.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.StateBasedSentenceBreaker.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.StateBasedSentenceBreaker.__new__": "text.BertTokenizer.__new__",
    "text.Tokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.Tokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.Tokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.Tokenizer.__init__": "text.Detokenizer.__init__",
    "text.Tokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.Tokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.Tokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.Tokenizer.__new__": "text.BertTokenizer.__new__",
    "text.TokenizerWithOffsets.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.TokenizerWithOffsets.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.TokenizerWithOffsets.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.TokenizerWithOffsets.__init__": "text.Detokenizer.__init__",
    "text.TokenizerWithOffsets.__le__": "text.keras.layers.ToDense.__le__",
    "text.TokenizerWithOffsets.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.TokenizerWithOffsets.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.TokenizerWithOffsets.__new__": "text.BertTokenizer.__new__",
    "text.TokenizerWithOffsets.split": "text.Tokenizer.split",
    "text.TokenizerWithOffsets.tokenize": "text.Tokenizer.tokenize",
    "text.UnicodeCharTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.UnicodeCharTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.UnicodeCharTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.UnicodeCharTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.UnicodeCharTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.UnicodeCharTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.UnicodeCharTokenizer.__new__": "text.BertTokenizer.__new__",
    "text.UnicodeCharTokenizer.split": "text.Tokenizer.split",
    "text.UnicodeCharTokenizer.split_with_offsets": "text.TokenizerWithOffsets.split_with_offsets",
    "text.UnicodeScriptTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.UnicodeScriptTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.UnicodeScriptTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.UnicodeScriptTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.UnicodeScriptTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.UnicodeScriptTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.UnicodeScriptTokenizer.__new__": "text.BertTokenizer.__new__",
    "text.UnicodeScriptTokenizer.split": "text.Tokenizer.split",
    "text.UnicodeScriptTokenizer.split_with_offsets": "text.TokenizerWithOffsets.split_with_offsets",
    "text.WaterfallTrimmer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.WaterfallTrimmer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.WaterfallTrimmer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.WaterfallTrimmer.__le__": "text.keras.layers.ToDense.__le__",
    "text.WaterfallTrimmer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.WaterfallTrimmer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.WaterfallTrimmer.__new__": "text.BertTokenizer.__new__",
    "text.WaterfallTrimmer.trim": "text.RoundRobinTrimmer.trim",
    "text.WhitespaceTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.WhitespaceTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.WhitespaceTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.WhitespaceTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.WhitespaceTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.WhitespaceTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.WhitespaceTokenizer.__new__": "text.BertTokenizer.__new__",
    "text.WhitespaceTokenizer.split": "text.Tokenizer.split",
    "text.WhitespaceTokenizer.split_with_offsets": "text.TokenizerWithOffsets.split_with_offsets",
    "text.WordpieceTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.WordpieceTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.WordpieceTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.WordpieceTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.WordpieceTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.WordpieceTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.WordpieceTokenizer.__new__": "text.BertTokenizer.__new__",
    "text.WordpieceTokenizer.split": "text.Tokenizer.split",
    "text.WordpieceTokenizer.split_with_offsets": "text.TokenizerWithOffsets.split_with_offsets"
  },
  "is_fragment": {
    "text": false,
    "text.BertTokenizer": false,
    "text.BertTokenizer.__eq__": true,
    "text.BertTokenizer.__ge__": true,
    "text.BertTokenizer.__gt__": true,
    "text.BertTokenizer.__init__": true,
    "text.BertTokenizer.__le__": true,
    "text.BertTokenizer.__lt__": true,
    "text.BertTokenizer.__ne__": true,
    "text.BertTokenizer.__new__": true,
    "text.BertTokenizer.detokenize": true,
    "text.BertTokenizer.split": true,
    "text.BertTokenizer.split_with_offsets": true,
    "text.BertTokenizer.tokenize": true,
    "text.BertTokenizer.tokenize_with_offsets": true,
    "text.Detokenizer": false,
    "text.Detokenizer.__eq__": true,
    "text.Detokenizer.__ge__": true,
    "text.Detokenizer.__gt__": true,
    "text.Detokenizer.__init__": true,
    "text.Detokenizer.__le__": true,
    "text.Detokenizer.__lt__": true,
    "text.Detokenizer.__ne__": true,
    "text.Detokenizer.__new__": true,
    "text.Detokenizer.detokenize": true,
    "text.FirstNItemSelector": false,
    "text.FirstNItemSelector.__eq__": true,
    "text.FirstNItemSelector.__ge__": true,
    "text.FirstNItemSelector.__gt__": true,
    "text.FirstNItemSelector.__init__": true,
    "text.FirstNItemSelector.__le__": true,
    "text.FirstNItemSelector.__lt__": true,
    "text.FirstNItemSelector.__ne__": true,
    "text.FirstNItemSelector.__new__": true,
    "text.FirstNItemSelector.get_selectable": true,
    "text.FirstNItemSelector.get_selection_mask": true,
    "text.FirstNItemSelector.unselectable_ids": true,
    "text.HubModuleSplitter": false,
    "text.HubModuleSplitter.__eq__": true,
    "text.HubModuleSplitter.__ge__": true,
    "text.HubModuleSplitter.__gt__": true,
    "text.HubModuleSplitter.__init__": true,
    "text.HubModuleSplitter.__le__": true,
    "text.HubModuleSplitter.__lt__": true,
    "text.HubModuleSplitter.__ne__": true,
    "text.HubModuleSplitter.__new__": true,
    "text.HubModuleSplitter.split": true,
    "text.HubModuleSplitter.split_with_offsets": true,
    "text.HubModuleTokenizer": false,
    "text.HubModuleTokenizer.__eq__": true,
    "text.HubModuleTokenizer.__ge__": true,
    "text.HubModuleTokenizer.__gt__": true,
    "text.HubModuleTokenizer.__init__": true,
    "text.HubModuleTokenizer.__le__": true,
    "text.HubModuleTokenizer.__lt__": true,
    "text.HubModuleTokenizer.__ne__": true,
    "text.HubModuleTokenizer.__new__": true,
    "text.HubModuleTokenizer.split": true,
    "text.HubModuleTokenizer.split_with_offsets": true,
    "text.HubModuleTokenizer.tokenize": true,
    "text.HubModuleTokenizer.tokenize_with_offsets": true,
    "text.MaskValuesChooser": false,
    "text.MaskValuesChooser.__eq__": true,
    "text.MaskValuesChooser.__ge__": true,
    "text.MaskValuesChooser.__gt__": true,
    "text.MaskValuesChooser.__init__": true,
    "text.MaskValuesChooser.__le__": true,
    "text.MaskValuesChooser.__lt__": true,
    "text.MaskValuesChooser.__ne__": true,
    "text.MaskValuesChooser.__new__": true,
    "text.MaskValuesChooser.get_mask_values": true,
    "text.MaskValuesChooser.mask_token": true,
    "text.MaskValuesChooser.random_token_rate": true,
    "text.MaskValuesChooser.vocab_size": true,
    "text.RandomItemSelector": false,
    "text.RandomItemSelector.__eq__": true,
    "text.RandomItemSelector.__ge__": true,
    "text.RandomItemSelector.__gt__": true,
    "text.RandomItemSelector.__init__": true,
    "text.RandomItemSelector.__le__": true,
    "text.RandomItemSelector.__lt__": true,
    "text.RandomItemSelector.__ne__": true,
    "text.RandomItemSelector.__new__": true,
    "text.RandomItemSelector.get_selectable": true,
    "text.RandomItemSelector.get_selection_mask": true,
    "text.RandomItemSelector.max_selections_per_batch": true,
    "text.RandomItemSelector.selection_rate": true,
    "text.RandomItemSelector.shuffle_fn": true,
    "text.RandomItemSelector.unselectable_ids": true,
    "text.Reduction": false,
    "text.Reduction.MEAN": true,
    "text.Reduction.STRING_JOIN": true,
    "text.Reduction.SUM": true,
    "text.RegexSplitter": false,
    "text.RegexSplitter.__eq__": true,
    "text.RegexSplitter.__ge__": true,
    "text.RegexSplitter.__gt__": true,
    "text.RegexSplitter.__init__": true,
    "text.RegexSplitter.__le__": true,
    "text.RegexSplitter.__lt__": true,
    "text.RegexSplitter.__ne__": true,
    "text.RegexSplitter.__new__": true,
    "text.RegexSplitter.split": true,
    "text.RegexSplitter.split_with_offsets": true,
    "text.RoundRobinTrimmer": false,
    "text.RoundRobinTrimmer.__eq__": true,
    "text.RoundRobinTrimmer.__ge__": true,
    "text.RoundRobinTrimmer.__gt__": true,
    "text.RoundRobinTrimmer.__init__": true,
    "text.RoundRobinTrimmer.__le__": true,
    "text.RoundRobinTrimmer.__lt__": true,
    "text.RoundRobinTrimmer.__ne__": true,
    "text.RoundRobinTrimmer.__new__": true,
    "text.RoundRobinTrimmer.generate_mask": true,
    "text.RoundRobinTrimmer.trim": true,
    "text.SentencepieceTokenizer": false,
    "text.SentencepieceTokenizer.__eq__": true,
    "text.SentencepieceTokenizer.__ge__": true,
    "text.SentencepieceTokenizer.__gt__": true,
    "text.SentencepieceTokenizer.__init__": true,
    "text.SentencepieceTokenizer.__le__": true,
    "text.SentencepieceTokenizer.__lt__": true,
    "text.SentencepieceTokenizer.__ne__": true,
    "text.SentencepieceTokenizer.__new__": true,
    "text.SentencepieceTokenizer.detokenize": true,
    "text.SentencepieceTokenizer.id_to_string": true,
    "text.SentencepieceTokenizer.split": true,
    "text.SentencepieceTokenizer.split_with_offsets": true,
    "text.SentencepieceTokenizer.string_to_id": true,
    "text.SentencepieceTokenizer.tokenize": true,
    "text.SentencepieceTokenizer.tokenize_with_offsets": true,
    "text.SentencepieceTokenizer.vocab_size": true,
    "text.SplitMergeFromLogitsTokenizer": false,
    "text.SplitMergeFromLogitsTokenizer.__eq__": true,
    "text.SplitMergeFromLogitsTokenizer.__ge__": true,
    "text.SplitMergeFromLogitsTokenizer.__gt__": true,
    "text.SplitMergeFromLogitsTokenizer.__init__": true,
    "text.SplitMergeFromLogitsTokenizer.__le__": true,
    "text.SplitMergeFromLogitsTokenizer.__lt__": true,
    "text.SplitMergeFromLogitsTokenizer.__ne__": true,
    "text.SplitMergeFromLogitsTokenizer.__new__": true,
    "text.SplitMergeFromLogitsTokenizer.split": true,
    "text.SplitMergeFromLogitsTokenizer.split_with_offsets": true,
    "text.SplitMergeFromLogitsTokenizer.tokenize": true,
    "text.SplitMergeFromLogitsTokenizer.tokenize_with_offsets": true,
    "text.SplitMergeTokenizer": false,
    "text.SplitMergeTokenizer.__eq__": true,
    "text.SplitMergeTokenizer.__ge__": true,
    "text.SplitMergeTokenizer.__gt__": true,
    "text.SplitMergeTokenizer.__init__": true,
    "text.SplitMergeTokenizer.__le__": true,
    "text.SplitMergeTokenizer.__lt__": true,
    "text.SplitMergeTokenizer.__ne__": true,
    "text.SplitMergeTokenizer.__new__": true,
    "text.SplitMergeTokenizer.split": true,
    "text.SplitMergeTokenizer.split_with_offsets": true,
    "text.SplitMergeTokenizer.tokenize": true,
    "text.SplitMergeTokenizer.tokenize_with_offsets": true,
    "text.Splitter": false,
    "text.Splitter.__eq__": true,
    "text.Splitter.__ge__": true,
    "text.Splitter.__gt__": true,
    "text.Splitter.__init__": true,
    "text.Splitter.__le__": true,
    "text.Splitter.__lt__": true,
    "text.Splitter.__ne__": true,
    "text.Splitter.__new__": true,
    "text.Splitter.split": true,
    "text.SplitterWithOffsets": false,
    "text.SplitterWithOffsets.__eq__": true,
    "text.SplitterWithOffsets.__ge__": true,
    "text.SplitterWithOffsets.__gt__": true,
    "text.SplitterWithOffsets.__init__": true,
    "text.SplitterWithOffsets.__le__": true,
    "text.SplitterWithOffsets.__lt__": true,
    "text.SplitterWithOffsets.__ne__": true,
    "text.SplitterWithOffsets.__new__": true,
    "text.SplitterWithOffsets.split": true,
    "text.SplitterWithOffsets.split_with_offsets": true,
    "text.StateBasedSentenceBreaker": false,
    "text.StateBasedSentenceBreaker.__eq__": true,
    "text.StateBasedSentenceBreaker.__ge__": true,
    "text.StateBasedSentenceBreaker.__gt__": true,
    "text.StateBasedSentenceBreaker.__init__": true,
    "text.StateBasedSentenceBreaker.__le__": true,
    "text.StateBasedSentenceBreaker.__lt__": true,
    "text.StateBasedSentenceBreaker.__ne__": true,
    "text.StateBasedSentenceBreaker.__new__": true,
    "text.StateBasedSentenceBreaker.break_sentences": true,
    "text.StateBasedSentenceBreaker.break_sentences_with_offsets": true,
    "text.Tokenizer": false,
    "text.Tokenizer.__eq__": true,
    "text.Tokenizer.__ge__": true,
    "text.Tokenizer.__gt__": true,
    "text.Tokenizer.__init__": true,
    "text.Tokenizer.__le__": true,
    "text.Tokenizer.__lt__": true,
    "text.Tokenizer.__ne__": true,
    "text.Tokenizer.__new__": true,
    "text.Tokenizer.split": true,
    "text.Tokenizer.tokenize": true,
    "text.TokenizerWithOffsets": false,
    "text.TokenizerWithOffsets.__eq__": true,
    "text.TokenizerWithOffsets.__ge__": true,
    "text.TokenizerWithOffsets.__gt__": true,
    "text.TokenizerWithOffsets.__init__": true,
    "text.TokenizerWithOffsets.__le__": true,
    "text.TokenizerWithOffsets.__lt__": true,
    "text.TokenizerWithOffsets.__ne__": true,
    "text.TokenizerWithOffsets.__new__": true,
    "text.TokenizerWithOffsets.split": true,
    "text.TokenizerWithOffsets.split_with_offsets": true,
    "text.TokenizerWithOffsets.tokenize": true,
    "text.TokenizerWithOffsets.tokenize_with_offsets": true,
    "text.UnicodeCharTokenizer": false,
    "text.UnicodeCharTokenizer.__eq__": true,
    "text.UnicodeCharTokenizer.__ge__": true,
    "text.UnicodeCharTokenizer.__gt__": true,
    "text.UnicodeCharTokenizer.__init__": true,
    "text.UnicodeCharTokenizer.__le__": true,
    "text.UnicodeCharTokenizer.__lt__": true,
    "text.UnicodeCharTokenizer.__ne__": true,
    "text.UnicodeCharTokenizer.__new__": true,
    "text.UnicodeCharTokenizer.detokenize": true,
    "text.UnicodeCharTokenizer.split": true,
    "text.UnicodeCharTokenizer.split_with_offsets": true,
    "text.UnicodeCharTokenizer.tokenize": true,
    "text.UnicodeCharTokenizer.tokenize_with_offsets": true,
    "text.UnicodeScriptTokenizer": false,
    "text.UnicodeScriptTokenizer.__eq__": true,
    "text.UnicodeScriptTokenizer.__ge__": true,
    "text.UnicodeScriptTokenizer.__gt__": true,
    "text.UnicodeScriptTokenizer.__init__": true,
    "text.UnicodeScriptTokenizer.__le__": true,
    "text.UnicodeScriptTokenizer.__lt__": true,
    "text.UnicodeScriptTokenizer.__ne__": true,
    "text.UnicodeScriptTokenizer.__new__": true,
    "text.UnicodeScriptTokenizer.split": true,
    "text.UnicodeScriptTokenizer.split_with_offsets": true,
    "text.UnicodeScriptTokenizer.tokenize": true,
    "text.UnicodeScriptTokenizer.tokenize_with_offsets": true,
    "text.WaterfallTrimmer": false,
    "text.WaterfallTrimmer.__eq__": true,
    "text.WaterfallTrimmer.__ge__": true,
    "text.WaterfallTrimmer.__gt__": true,
    "text.WaterfallTrimmer.__init__": true,
    "text.WaterfallTrimmer.__le__": true,
    "text.WaterfallTrimmer.__lt__": true,
    "text.WaterfallTrimmer.__ne__": true,
    "text.WaterfallTrimmer.__new__": true,
    "text.WaterfallTrimmer.generate_mask": true,
    "text.WaterfallTrimmer.trim": true,
    "text.WhitespaceTokenizer": false,
    "text.WhitespaceTokenizer.__eq__": true,
    "text.WhitespaceTokenizer.__ge__": true,
    "text.WhitespaceTokenizer.__gt__": true,
    "text.WhitespaceTokenizer.__init__": true,
    "text.WhitespaceTokenizer.__le__": true,
    "text.WhitespaceTokenizer.__lt__": true,
    "text.WhitespaceTokenizer.__ne__": true,
    "text.WhitespaceTokenizer.__new__": true,
    "text.WhitespaceTokenizer.split": true,
    "text.WhitespaceTokenizer.split_with_offsets": true,
    "text.WhitespaceTokenizer.tokenize": true,
    "text.WhitespaceTokenizer.tokenize_with_offsets": true,
    "text.WordShape": false,
    "text.WordShape.BEGINS_WITH_OPEN_QUOTE": true,
    "text.WordShape.BEGINS_WITH_PUNCT_OR_SYMBOL": true,
    "text.WordShape.ENDS_WITH_CLOSE_QUOTE": true,
    "text.WordShape.ENDS_WITH_ELLIPSIS": true,
    "text.WordShape.ENDS_WITH_EMOTICON": true,
    "text.WordShape.ENDS_WITH_MULTIPLE_SENTENCE_TERMINAL": true,
    "text.WordShape.ENDS_WITH_MULTIPLE_TERMINAL_PUNCT": true,
    "text.WordShape.ENDS_WITH_PUNCT_OR_SYMBOL": true,
    "text.WordShape.ENDS_WITH_SENTENCE_TERMINAL": true,
    "text.WordShape.ENDS_WITH_TERMINAL_PUNCT": true,
    "text.WordShape.HAS_CURRENCY_SYMBOL": true,
    "text.WordShape.HAS_EMOJI": true,
    "text.WordShape.HAS_MATH_SYMBOL": true,
    "text.WordShape.HAS_MIXED_CASE": true,
    "text.WordShape.HAS_NON_LETTER": true,
    "text.WordShape.HAS_NO_DIGITS": true,
    "text.WordShape.HAS_NO_PUNCT_OR_SYMBOL": true,
    "text.WordShape.HAS_NO_QUOTES": true,
    "text.WordShape.HAS_ONLY_DIGITS": true,
    "text.WordShape.HAS_PUNCTUATION_DASH": true,
    "text.WordShape.HAS_QUOTE": true,
    "text.WordShape.HAS_SOME_DIGITS": true,
    "text.WordShape.HAS_SOME_PUNCT_OR_SYMBOL": true,
    "text.WordShape.HAS_TITLE_CASE": true,
    "text.WordShape.IS_ACRONYM_WITH_PERIODS": true,
    "text.WordShape.IS_EMOTICON": true,
    "text.WordShape.IS_LOWERCASE": true,
    "text.WordShape.IS_MIXED_CASE_LETTERS": true,
    "text.WordShape.IS_NUMERIC_VALUE": true,
    "text.WordShape.IS_PUNCT_OR_SYMBOL": true,
    "text.WordShape.IS_UPPERCASE": true,
    "text.WordShape.IS_WHITESPACE": true,
    "text.WordpieceTokenizer": false,
    "text.WordpieceTokenizer.__eq__": true,
    "text.WordpieceTokenizer.__ge__": true,
    "text.WordpieceTokenizer.__gt__": true,
    "text.WordpieceTokenizer.__init__": true,
    "text.WordpieceTokenizer.__le__": true,
    "text.WordpieceTokenizer.__lt__": true,
    "text.WordpieceTokenizer.__ne__": true,
    "text.WordpieceTokenizer.__new__": true,
    "text.WordpieceTokenizer.detokenize": true,
    "text.WordpieceTokenizer.split": true,
    "text.WordpieceTokenizer.split_with_offsets": true,
    "text.WordpieceTokenizer.tokenize": true,
    "text.WordpieceTokenizer.tokenize_with_offsets": true,
    "text.WordpieceTokenizer.vocab_size": true,
    "text.__version__": true,
    "text.case_fold_utf8": false,
    "text.coerce_to_structurally_valid_utf8": false,
    "text.combine_segments": false,
    "text.find_source_offsets": false,
    "text.gather_with_default": false,
    "text.greedy_constrained_sequence": false,
    "text.keras": false,
    "text.keras.layers": false,
    "text.keras.layers.ToDense": false,
    "text.keras.layers.ToDense.__eq__": true,
    "text.keras.layers.ToDense.__ge__": true,
    "text.keras.layers.ToDense.__gt__": true,
    "text.keras.layers.ToDense.__init__": true,
    "text.keras.layers.ToDense.__le__": true,
    "text.keras.layers.ToDense.__lt__": true,
    "text.keras.layers.ToDense.__ne__": true,
    "text.keras.layers.ToDense.__new__": true,
    "text.mask_language_model": false,
    "text.max_spanning_tree": false,
    "text.max_spanning_tree_gradient": false,
    "text.metrics": false,
    "text.metrics.rouge_l": false,
    "text.ngrams": false,
    "text.normalize_utf8": false,
    "text.normalize_utf8_with_offsets_map": false,
    "text.pad_along_dimension": false,
    "text.pad_model_inputs": false,
    "text.regex_split": false,
    "text.regex_split_with_offsets": false,
    "text.sentence_fragments": false,
    "text.sliding_window": false,
    "text.span_alignment": false,
    "text.span_overlaps": false,
    "text.viterbi_constrained_sequence": false,
    "text.wordshape": false
  },
  "py_module_names": [
    "text"
  ],
  "site_link": null
}
