name: On Host Tests
description: Runs on-host tests.
inputs:
  test_artifacts_key:
    description: "Artifact key used to store test artifacts."
    required: true
  test_results_key:
    description: "Artifact key used to store test results."
    required: true
  num_gtest_shards:
    description: "Total number of shards used to run the steps in this file."
    required: true
runs:
  using: "composite"
  steps:
    - name: Download Artifacts
      uses: actions/download-artifact@v4
      with:
        name: ${{ inputs.test_artifacts_key }}
    - name: Extract Artifacts
      shell: bash
      run: |
        set -x
        mkdir -p unit_test/
        cd unit_test/
        tar -I 'zstd -T0' -xf ../test_artifacts.tar.zstd
    - name: Run Tests
      id: run-tests
      shell: bash
      env:
        XVFB_SERVER_ARGS: "-screen 0 1920x1080x24i +render +extension GLX -noreset"
      run: |
        set -x
        env

        out_dir="${GITHUB_WORKSPACE}/unit_test/out/${{ matrix.platform}}_${{ matrix.config }}"
        # TODO: LD_LIBRARY_PATH should not include the starboard subdirectory.
        LD_LIBRARY_PATH="${out_dir}/starboard"
        LD_LIBRARY_PATH="${LD_LIBRARY_PATH}:${out_dir}"
        export LD_LIBRARY_PATH

        # TODO: b/467135717 - Remove this once tests don't have asan leaks.
        # Don't fail if only asan fails.
        export ASAN_OPTIONS=exitcode=0

        # Make results dir available to the archiving step below.
        results_dir="${GITHUB_WORKSPACE}/results"
        echo "results_dir=${results_dir}" >> $GITHUB_ENV

        # Test results (xml and logs) must be in a subfolder in results_dir
        # to be picked up by the test result processor.
        # This also prevents accidental overwriting when collecting results.
        test_output="${results_dir}/shard_${{ matrix.shard }}"
        mkdir -p ${test_output}

        failed_suites=""
        cd unit_test/

        for test_binary_path in $(cat out/${{ matrix.platform }}_${{ matrix.config }}/test_targets.json | jq -cr '.executables | join(" ")'); do
          filename=$(basename "${test_binary_path}")
          test_binary="${filename%%.*}"

          if [[ "${{ matrix.shard }}" != "0" && "${test_binary}" =~ ^nplb ]]; then
            # Workaround for nplb not supporting sharding.
            continue
          fi

          echo "Running tests for suite: ${test_binary}"

          test_filter="*"
          test_filter_json_dir="${GITHUB_WORKSPACE}/cobalt/testing/filters/${{ matrix.platform }}/${test_binary}_filter.json"
          if [ -f ${test_filter_json_dir} ]; then
            test_filter=`jq -r '"-" + (.failing_tests | join(":"))' ${test_filter_json_dir}`
          fi

          echo "Test filter evaluated to: ${test_filter}"
          xml_path="${test_output}/${test_binary}_result.xml"
          log_path="${test_output}/${test_binary}_log.txt"
          # TODO: Investigate test_runner.py

          ENTRYPOINT=./"${test_binary_path}"

          if [ "${test_filter}" == "-*" ]; then
            echo "Skipped due to test filter." >  ${log_path}
          else
            export GTEST_TOTAL_SHARDS=${{ inputs.num_gtest_shards }}
            export GTEST_SHARD_INDEX=${{ matrix.shard }}

            crash_filter="-crash_detector"
            for i in {1..100}; do
              set +e
              /usr/bin/xvfb-run -a --server-args="${XVFB_SERVER_ARGS}" \
                  stdbuf -i0 -o0 -e0 $ENTRYPOINT \
                  --single-process-tests \
                  --gtest_output="xml:${xml_path}" \
                  --gtest_filter="${test_filter}:${crash_filter}" 2>&1 | tee ${log_path} || {
                  failed_suites="${failed_suites} ${test_binary}"
              }

              if [[ ! -f ${xml_path} ]]; then
                # Test binary crashed. Amend test filter and retry
                crashed_test=$(grep "\[ RUN      \]" "${log_path}" | tail -n 1 | awk '{print $NF}')
                if [[ -z "${crashed_test}" ]]; then
                  echo "Extraction failed or crash before tests started."
                  break
                fi
                crash_filter="${crash_filter}:-${crashed_test}"
                echo "Retrying with filter: '${crash_filter}'"
                echo "${crash_filter}" > "${test_output}/${test_binary}_crash.filter"
              else
                break
              fi
            done
            if [[ ! -f ${xml_path} ]]; then
              # Test binary crashed. Generate a fake JUnit XML report with the last run test.
              python3 ${GITHUB_WORKSPACE}/.github/scripts/generate_crash_report.py "${log_path}" > "${xml_path}"
            fi
          fi
        done
        echo "Finished running tests..."
        if [[ -n "${failed_suites}" ]]; then
          echo "Test suites failed:${failed_suites}"
          # Fail the job so it's easy to retrigger.
          exit 1
        fi
    - name: Archive Test Results
      if: success() || failure()
      uses: actions/upload-artifact@v4
      with:
        name: ${{ inputs.test_results_key }}
        path: ${{ env.results_dir }}/*
