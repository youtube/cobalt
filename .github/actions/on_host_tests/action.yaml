name: On Host Tests
description: Runs on-host tests.
inputs:
  test_artifacts_key:
    description: "Artifact key used to store test artifacts."
    required: true
  test_results_key:
    description: "Artifact key used to store test results."
    required: true
  num_gtest_shards:
    description: "Total number of shards used to run the steps in this file."
    required: true
runs:
  using: "composite"
  steps:
    - name: Download Artifacts
      uses: actions/download-artifact@v4
      with:
        name: ${{ inputs.test_artifacts_key }}
    - name: Extract Artifacts
      shell: bash
      run: |
        set -x
        mkdir -p unit_test/
        cd unit_test/
        tar xzf ../test_artifacts.tar.gz
    - name: Run Tests
      id: run-tests
      shell: bash
      env:
        XVFB_SERVER_ARGS: "-screen 0 1920x1080x24i +render +extension GLX -noreset"
      run: |
        set -x
        env

        out_dir="${GITHUB_WORKSPACE}/unit_test/out/${{ matrix.platform}}_${{ matrix.config }}"
        # TODO: LD_LIBRARY_PATH should not include the starboard subdirectory.
        LD_LIBRARY_PATH="${LD_LIBRARY_PATH}:${out_dir}"
        export LD_LIBRARY_PATH

        # Make results dir available to the archiving step below.
        results_dir="${GITHUB_WORKSPACE}/results"
        echo "results_dir=${results_dir}" >> $GITHUB_ENV

        # Test results (xml and logs) must be in a subfolder in results_dir
        # to be picked up by the test result processor.
        # This also prevents accidental overwriting when collecting results.
        test_output="${results_dir}/shard_${{ matrix.shard }}"
        mkdir -p ${test_output}

        failed_suites=""
        cd unit_test/

        for test_binary_path in $(cat out/${{ matrix.platform }}_${{ matrix.config }}/test_targets.json | jq -cr '.executables | join(" ")'); do
          filename=$(basename "${test_binary_path}")
          test_binary="${filename%%.*}"
          echo "Running tests for suite: ${test_binary}"

          ENTRYPOINT=./"${test_binary_path}"

          # Combine the shard filter with the exclusion filter from the JSON file.
          # First, get the exclusion filter.
          test_filter_from_file="*"
          test_filter_json_dir="${GITHUB_WORKSPACE}/cobalt/testing/filters/${{ matrix.platform }}/${test_binary}_filter.json"
          if [ -f ${test_filter_json_dir} ]; then
            test_filter_from_file=`jq -r '"-" + (.failing_tests | join(":"))' ${test_filter_json_dir}`
          fi

          # Second, get the inclusion filter for this specific shard.
          # Get the full list of tests from the binary.
          all_tests=$($ENTRYPOINT --gtest_list_tests)
          # Use the helper script to get the specific filter for this shard.
          shard_filter=$(echo "${all_tests}" | python3 ${GITHUB_WORKSPACE}/.github/scripts/shard_tests.py ${{ inputs.num_gtest_shards }} ${{ matrix.shard }})

          # Third, combine them.
          if [ "${test_filter_from_file}" == "-*" ]; then
            # If all tests are excluded by the file, that takes precedence.
            test_filter="${test_filter_from_file}"
          elif [ -z "${shard_filter}" ]; then
            # If the shard filter is empty, it means this shard has no tests to run.
            # We can use a filter that matches nothing.
            test_filter="-"
          else
            # Prepend the shard's tests to the exclusion list.
            # e.g. "Test.A:Test.B:-Failing.Test:Failing.Test2"
            test_filter="${shard_filter}${test_filter_from_file}"
          fi

          echo "Test filter evaluated to: ${test_filter}"
          xml_path="${test_output}/${test_binary}_result.xml"
          log_path="${test_output}/${test_binary}_log.txt"
          # TODO: Investigate test_runner.py

          if [ "${test_filter}" == "-*" ] || [ "${test_filter}" == "-" ]; then
            echo "Skipped due to test filter." >  ${log_path}
          else
            /usr/bin/xvfb-run -a --server-args="${XVFB_SERVER_ARGS}" \
                stdbuf -i0 -o0 -e0 $ENTRYPOINT \
                --single-process-tests \
                --gtest_output="xml:${xml_path}" \
                --gtest_filter="${test_filter}" 2>&1 | tee ${log_path} || {
              failed_suites="${failed_suites} ${test_binary}"
            }

            if [[ ! -f ${xml_path} ]]; then
              # Test binary crashed. Generate a fake JUnit XML report with the last run test.
              python3 ${GITHUB_WORKSPACE}/.github/scripts/generate_crash_report.py "${log_path}" "${xml_path}"
            fi
          fi
        done
        echo "Finished running tests..."
        if [[ -n "${failed_suites}" ]]; then
          echo "Test suites failed:${failed_suites}"
          # Fail the job so it's easy to retrigger.
          exit 1
        fi
    - name: Archive Test Results
      if: success() || failure()
      uses: actions/upload-artifact@v4
      with:
        name: ${{ inputs.test_results_key }}
        path: ${{ env.results_dir }}/*
