// Copyright 2023 The ODML Authors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//      http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

syntax = "proto3";

package odml.infra.proto;

option java_package = "com.google.odml.infra.proto";
option java_outer_classname = "TransformerParametersProto";

// The parameters of transformer (https://arxiv.org/pdf/1706.03762.pdf)
// Next ID: 41
message TransformerParameters {
  // Batch size of tensors.
  int32 batch_size = 1;

  // TODO: b/319312256 - Deprecate parameter.
  // Maximum sequence length of the input/output tensor.
  int32 max_seq_length = 2;

  // Embedding dimension (or model dimension), `d_model` in the paper.
  // `d_k` == `d_v` == `d_model`/`h`.
  int32 embedding_dim = 3;

  // Hidden dimension used in the feedforward layer, `d_ff` in the paper.
  int32 hidden_dimension = 4;

  // Head dimension, `d_k` or `d_v` in the paper.
  int32 head_dimension = 5;

  // Number of heads, `h` in the paper.
  int32 num_heads = 6;

  // Number of stacked transformers, `N` in the paper.
  int32 num_stacks = 7;

  // Deprecated: bool use_mqa. Use num_kv_heads below.
  reserved 8;

  // Number of kv heads. 0 means Multi-Head-Attention (MHA), key and value have
  // same number of heads as query; 1 means Multi-Query-Attention (MQA), key and
  // value have one head; otherwise, this specifies the number of heads for key
  // and value, and Grouped-Query-Attention (GQA) will be used. See
  // https://arxiv.org/pdf/2305.13245.pdf for details.
  int32 num_kv_heads = 9;

  // Different types of attention mask type.
  enum AttentionMaskType {
    UNSPECIFIED = 0;
    CAUSAL = 1;
    PREFIX = 2;
    BIDIRECTIONAL = 3;
  }
  // Deprecated, use SelfAttentionParameters.
  reserved 10;

  enum Activation {
    ACTIVATION_UNSPECIFIED = 0;
    // GELU stands for Gaussian Error Linear Unit, see
    // https://arxiv.org/pdf/1606.08415.pdf for details.
    GELU = 1;
    // SILU stands for Sigmoid-Weighted Linear Unit, see
    // https://arxiv.org/pdf/1702.03118v3.pdf for details.
    SILU = 2;
    // RELU stands for Rectified Linear Unit, see
    // https://dl.acm.org/doi/10.5555/3104322.3104425 for details.
    RELU = 3;
    // The output of RELU is multiplied by its square root.
    RELU1P5 = 4;
  }

  enum Norm {
    NORM_UNSPECIFIED = 0;
    // No normalization operation will be perform.
    NO_NORM = 1;
    // RMSNORM stands for Root Mean Square Layer Normalization, see
    // https://arxiv.org/pdf/1910.07467.pdf for details.
    RMS_NORM = 2;
    // LAYERNORM stands for Layer Normalization, see
    // https://arxiv.org/pdf/1607.06450v1.pdf for details.
    LAYER_NORM = 3;
    // RMSNORM without scale. This is equivalent to setting all scales to 0.
    RMS_NORM_NO_SCALE = 4;
  }

  message FeedForwardParameters {
    // If `no_bias`, fully connect will degrade to matrix multiply.
    bool no_bias = 1;
    Activation activation = 2;
    // Normalization before the dense layer.
    Norm pre_norm = 3;
    // Normalization after the dense layer.
    Norm post_norm = 4;
  }

  FeedForwardParameters feed_forward_parameters = 11;

  message FinalProjectParameters {
    // If `no_bias`, fully connect will degrade to matrix multiply.
    bool no_bias = 1;

    // The value to set the soft cap (Tanh) before calling the final project
    // layer. Setting the value to be <=0 indicates there is no cap.
    float soft_cap_value = 2;
  }

  FinalProjectParameters final_project_parameters = 12;

  // Normalization before the transformer block.
  Norm pre_norm = 13;
  // Normalization after the transformer block.
  Norm post_norm = 14;
  Norm final_norm = 15;

  enum AttentionScaleType {
    SCALE_TYPE_UNSPECIFIED = 0;

    // Per dimension scale, query is scaled by log_2(1 + exp(w)) /
    // sqrt(head_dim) where w is s static weight.
    SCALE_TYPE_PER_DIM_SCALE = 1;

    // Query is scaled by 1/sqrt(head_dim).
    SCALE_TYPE_INV_SQRT_HEAD_DIM = 2;

    // Query is scaled by 1/sqrt(model_dim/num_heads)
    // model_dim/num_heads is not always equal to head_dim
    SCALE_TYPE_INV_SQRT_D_MODEL_DIV_NUM_HEADS = 3;

    // Query is scaled by query_rescale_factor/ head_dim.
    SCALE_TYPE_RESCALE_FACTOR_INV_HEAD_DIM = 4;
  }

  message SelfAttentionParameters {
    // Whether bias term is used in Q, K, and V projections.
    bool qkv_no_bias = 1;
    // Whether bias term is used in post-projection.
    bool post_proj_no_bias = 2;

    AttentionMaskType attention_mask_type = 3;

    // The value to set the soft cap (Tanh) before calling the attention
    // softmax. Setting the value to be <=0 indicates there is no cap.
    float soft_cap_value = 4;

    // If specified, inference pipeline will use the specified scale type.
    // Otherwise SCALE_TYPE_PER_DIM_SCALE is used for Multi-Query-Attention by
    // default, and SCALE_TYPE_INV_SQRT_HEAD_DIM is used for
    // Multi-Head-Attention by default.
    optional AttentionScaleType attention_scale_type = 5;

    // If set, determines how many tokens in the context history (before the
    // current time step) are used for the self-attention calculation.
    // Otherwise, the full context history is used.
    optional int32 sliding_window_size = 6;

    // If set, enables separate q and k weighted norms in the attention layer
    // (used for Gemma3).
    optional bool qk_norm = 7;
  }

  SelfAttentionParameters self_attention_parameters = 16;

  reserved 39;

  reserved 17;
  // Whether to skip absolute positional embeddings. If the value is false, then
  // the absolute positional embeddings will be applied to the token embeddings
  // before the attention.
  bool skip_absolute_positional_embeddings = 18;

  reserved 20, 21, 23, 24;

  // If set, will override the default RoPE scaling factor to be the specified
  // value for all global layers (but will not change local attention layers).
  optional float global_rope_scaling = 33;

  // The number of local attention layers for each global attention layer. For
  // example, if the value is 3, then the stacked transformer will have the
  // pattern L-L-L-G-L-L-L-G- ... Note that if the value is 0, then the stacked
  // transformer will only have global attention layers.
  int32 num_local_layers_per_global = 19;

  // If set, will override the default RoPE wavelength to be the specified
  // value for all global layers (but will not change local attention layers).
  optional float global_rope_wavelength = 32;

  reserved 40;

  // Audio parameters
  // Describes where to interleave residual adapters with transformer layers.
  enum WhereToInterleave {
    // Add a residual adapter after every transformer layer.
    INTERLEAVE_UNSPECIFIED = 0;
    ALL = 1;
    // Add a residual adapter after every 4th transformer layer, starting with
    // index 0 (i.e. after 0, 4, 8, ...).
    EVERY_OTHER_4 = 2;
  }

  // Holds information on audio residual adapters
  message ResidualAdapterParameters {
    WhereToInterleave where_to_interleave = 1;
    // bottleneck_dim is akin to hidden_dim in the FF layer
    int32 bottleneck_dimension = 2;
  }

  optional ResidualAdapterParameters residual_adapter_parameters = 22;

  // Scale the queries by this value.
  optional float query_rescale_factor = 25;

  // Vision parameters
  int32 vision_tokens_num = 26;
  int32 max_num_images = 28;

  // The number of stacks that are treated as "extra", which may have slightly
  // different loading behavior.
  int32 num_extra_stacks = 27;

  // Audio parameters
  // Which index audio hard tokens start at.
  int32 audio_vocab_offset = 29;
  // Length of audio_input_embedding_table_extra variable.
  int32 audio_vocab_extra_dim = 30;
  // Dimension which we embed audio hard tokens to.
  int32 audio_input_embedding_dim = 31;

  // Laurel configs.
  message LaurelConfig {
    // The embedding w/ shape [B, L, d_model] will first be projected to
    // [B, L, laurel_rank] and then projected to [B, L, d_model].
    optional int32 laurel_rank = 1;
    // The scale shift for the post laurel rms normalization. The default value
    // is 0.0f, which means no scale shift.
    optional float post_laurel_norm_scale_shift = 2;
  }
  optional LaurelConfig laurel_config = 34;

  // The start layer of the kv cache that is shared from the previous layers.
  optional int32 kv_cache_shared_start_layer = 35;

  // Gemma3 models are trained from bf16, so they are susceptible to overflow
  // when run in environments without native bf16 support (especially for larger
  // models, like 4B+). Thus, to ensure correct behavior for these models when
  // not forcing fp32 precision, we allow for applying a special workaround.
  optional bool gemma3_bfloat16_fix = 36;

  // Altup configs.
  message AltupConfig {
    // The number of modalities for altup.
    optional int32 altup_num_modalities = 1;
    // The index of the active modality for altup.
    optional int32 altup_active_idx = 2;
    // Number of inputs.
    optional int32 altup_num_inputs = 3;
  }
  optional AltupConfig altup_config = 37;

  // Activation sparsity stddev multiplier. It depends on the specified sparsity
  // probability. In particular, stddev_multiplier = cdf^{-1}(sparsity_prob),
  // where cdf is the cumulative distribution function of the Gaussian
  // distribution.
  optional float activation_sparsity_stddev_multiplier = 38;
}
