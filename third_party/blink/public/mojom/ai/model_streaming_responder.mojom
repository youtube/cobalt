// Copyright 2024 The Chromium Authors
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

module blink.mojom;

// The status of the ModelStreamingResponder response.
// TODO(leimy): return more information about the erroneous case.
// These values are persisted to logs. Entries should not be renumbered and
// numeric values should never be reused.
//
// LINT.IfChange(ModelStreamingResponseStatus)
enum ModelStreamingResponseStatus {
  // There response is not fully streamed back yet, and the interface will
  // remain open.
  kOngoing = 0,
  // The ModelStreamingResponder completes and closes.
  kComplete = 1,
  // The following enums are for the case when the ModelStreamingResponder closes with some errors.
  kErrorUnknown = 2,
  // The request was invalid.
  kErrorInvalidRequest = 3,
  // The request was throttled.
  kErrorRequestThrottled = 4,
  // User permission errors such as not signed-in or not allowed to execute
  // model.
  kErrorPermissionDenied = 5,
  // Other generic failures.
  kErrorGenericFailure = 6,
  // Retryable error occurred in server.
  kErrorRetryableError = 7,
  // Non-retryable error occurred in server.
  kErrorNonRetryableError = 8,
  // Unsupported language.
  kErrorUnsupportedLanguage = 9,
  // Request was filtered.
  kErrorFiltered = 10,
  // Response was disabled.
  kErrorDisabled = 11,
  // The request was cancelled.
  kErrorCancelled = 12,
  // The session has been destroyed.
  kErrorSessionDestroyed = 13,
  // The prompt is too large to be stored in the context.
  kErrorPromptRequestTooLarge = 14,
  // Response was retracted due to low quality.
  kErrorResponseLowQuality = 15,
  // Append new items here.
};
// LINT.ThenChange(//tools/metrics/histograms/metadata/ai/enums.xml:AIModelStreamingResponseStatus)

// The struct stores the information about the context for the session.
struct ModelExecutionContextInfo {
  // The latest number of existing tokens.
  uint64 current_tokens;
};

// The enum describes how the responder should handle the latest chunk of
// data that is streamed back.
enum ModelStreamingResponderAction {
  // The data should completely replace the existing result.
  kReplace = 0,
  // The data should be appended to the existing result.
  kAppend = 1,
};

// The responder provides methods for the session to return the execution
// response in a streaming manner.
// It will be passed to the AIManager by the built-in AI interfaces
// (e.g. AILanguageModel, AISummarizer etc.) when they executes the model to
// get the response back.
interface ModelStreamingResponder {
  // Called when the model execution encounters errors.
  OnError(ModelStreamingResponseStatus status);
  // Called when there is a new chunk of data available for streaming.
  // Depending on the execution feature, the `text` may contain either the
  // latest chunk of the data, or the latest full response. How the responder
  // should handle the data will be indicated in the `action` field.
  OnStreaming(string text, ModelStreamingResponderAction action);
  // Called when the model execution completes. The optional `context_info`
  // contain information about the session like the current number of tokens
  // and if the context overflows.
  // It will only be set when necessary, for example, in the `AILanguageModel`
  // interface as it maintains the context of the past model execution.
  OnCompletion(ModelExecutionContextInfo? context_info);
  // Called when the `AILanguageModel` evicts some items from the context in
  // order to make space for the newly added items.
  // This might be called multiple times after prompting, but it won't happen
  // after `OnCompletion()` or `OnError()`.
  OnContextOverflow();
};
