// Copyright 2023 The Chromium Authors
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

module webnn.mojom;

import "mojo/public/mojom/base/big_buffer.mojom";

// Represents the `MLOperand` which describes not only input and constant
// operand, but also the output operand of operator.
struct Operand {
  // Represents the `MLOperandType` in the WebIDL definition.
  enum DataType {
    kFloat32,
    kFloat16,
    kInt32,
    kUint32,
    kInt8,
    kUint8,
  };

  enum Kind { kInput, kConstant, kOutput };

  Kind kind;
  // The data type of the operand.
  DataType data_type;
  // The dimensions of the operand.
  array<uint32> dimensions;
  // The name field is only required for input/output operands of graph.
  string? name;
};

// Clamp the input tensor element-wise within a range specified by the minimum
// and maximum values.
struct Clamp {
  // The id of input operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 input_operand_id;
  // The id of output operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 output_operand_id;

  // The minimum value of the range.
  float min_value;
  // The maximum value of the range.
  float max_value;
};

// Represents the concat operation that concatenates the input tensors along
// the given axis.
struct Concat {
  // The ids of input operand are used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  array<uint64> input_operand_ids;
  // The id of output operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 output_operand_id;
  // The axis used to concatenate along.
  uint32 axis;
};

// Represents the `MLInputOperandLayout` that specifies the layout format of
// the input tensor. `kChannelsFirst` means `nchw` (batches, channels, height,
// width), `kChannelsLast` means `nhwc` (batches, height, width, channels).
// The type is used to get the spatial dimension from input tensor, thus safe to
// represent as enum.
enum InputOperandLayout { kChannelsFirst, kChannelsLast };

// A size has height and width values.
struct Size2d {
  uint32 height;
  uint32 width;
};

// The additional rows and columns added to the beginning and ending of each
// spatial dimension of input.
struct Padding2d {
  // The height and width padding at the beginning of input tensor.
  Size2d beginning;
  // The height and width padding at the ending of input tensor.
  Size2d ending;
};

// Represents a 2-D convolution given the input and filter tensors.
//
// Only `oihw` (output_channels, input_channels/groups, height, width) filter
// layout is supported, other variants are being discussed in the working group
// https://github.com/webmachinelearning/webnn/issues/324.
struct Conv2d {
  // The id of input operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 input_operand_id;
  // The id of filter operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 filter_operand_id;
  // The id of output operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 output_operand_id;

  // The padding of input tensor which is the explicit pad or calculated pad
  // with `MLAutoPad` type, input sizes, filter size, strides and dilations in
  // blink side.
  Padding2d padding;
  // The stride of the sliding window for each spatial dimension of input.
  Size2d strides;
  // The dilation factor for each spatial dimension of input.
  Size2d dilations;
  // The number of groups that input channels and output channels are divided
  // into.
  uint32 groups = 1;
  // The layout format of the input.
  InputOperandLayout input_layout;
  // The additional 1-D tensor with the shape of output channels whose values
  // are added to the convolution result.
  uint64? bias_operand_id;
  // The optional activation function that immediately follows the convolution.
  Activation? activation;
};

// Represents an element-wise binary operation, mathematically equivalent to:
// <output_operand> = <lhs_operand> <operation_kind> <rhs_operand>;
// The shapes of left-hand side (lhs) operand and right-hand side (rhs) operand
// must be compatible according to numpy-broadcasting-rule:
// https://www.w3.org/TR/webnn/#biblio-numpy-broadcasting-rule
struct ElementWiseBinary  {
  enum Kind {
    kAdd,
    kSub,
    kMul,
    kDiv,
    kMax,
    kMin,
    kPow,
  };

  // The kind of binary operation.
  Kind kind;
  // The id of lhs operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 lhs_operand;
  // The id of rhs operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 rhs_operand;
  // The id of output operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 output_operand;
};

// Specifies the different ways to pad a tensor. The padding value is only
// specified when the mode is "constant".
struct ConstantPadding {
  float value = 0;
};
struct EdgePadding {};
struct ReflectionPadding {};
struct SymmetricPadding {};

union PaddingMode {
  ConstantPadding constant;
  EdgePadding edge;
  ReflectionPadding reflection;
  SymmetricPadding symmetric;
};

// Represents a pad operation which inflates the input tensor with constant or
// mirrored values on the edges.
struct Pad {
  // The id of input operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 input_operand_id;
  // The id of output operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 output_operand_id;
  // The number of padding values to add at the beginning of each input
  // dimension. The array length should be equal to the rank of input tensor.
  array<uint32> beginning_padding;
  // The number of padding values to add at the ending of each input
  // dimension. The array length should be equal to the rank of input tensor.
  array<uint32> ending_padding;

  PaddingMode mode;
};

// Represents an average or max pooling operation across all the elements with
// moving window over the input tensor.
// This struct also contains the attributes of pool2d operator, but the
// `roundingType` and `outputSizes` array in MLPool2dOptions are not included
// because they are used to calculate the output dimensions of pool2d in blink
// side.
struct Pool2d {
  enum Kind {
    kAveragePool2d,
    kMaxPool2d,
  };

  Kind kind;
  // The id of input operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 input_operand_id;
  // The id of output operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 output_operand_id;

  // The dimensions of the sliding window which is supplied by user or the
  // element of input operand height and width.
  Size2d window_dimensions;
  // The padding of input tensor which is the explicit pad or calculated pad
  // with `MLAutoPad` type, input sizes, window dimensions, strides and
  // dilations in blink side.
  Padding2d padding;
  // The element stride of the sliding window for each spatial dimension of
  // input.
  Size2d strides;
  // The dilation factor for each spatial dimension of input.
  Size2d dilations;
  // The layout format of the input.
  InputOperandLayout layout;
};

struct StartAndSize {
  uint32 start;
  uint32 size;
};

struct Slice {
  // The id of input operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 input_operand_id;
  // The id of output operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 output_operand_id;
  // An array containing the number of elements of the input window in each
  // dimension.
  array<StartAndSize> starts_and_sizes;
};

// Represents general matrix multiplication (gemm) operation in the expression
// `alpha * A * B + beta * C`.
struct Gemm {
  // The id of `A` operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 a_operand_id;
  // The id of `B` operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 b_operand_id;
  // The id of output operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 output_operand_id;

  // The id of optional `C` operand is used to get the `Operand` description
  // from `GraphInfo.id_to_operand_map`.
  uint64? c_operand_id;
  // A float scalar multiplier for the `A * B`.
  float alpha = 1.0;
  // A float scalar multiplier for the third tensor.
  float beta = 1.0;
  // True is to transpose the first tensor before matrix multiplication.
  bool a_transpose = false;
  // True is to transpose the second tensor before matrix multiplication.
  bool b_transpose = false;
};

// Represents a parametric relu operation whose calculation follows the
// expression max(0, x) + slope âˆ— min(0, x).
struct Prelu {
  // The id of input operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 input_operand_id;
  // The id of slope operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 slope_operand_id;
  // The id of output operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 output_operand_id;
};

// Corresponds to `MLOperand relu(MLOperand x)` that compute the rectified
// linear function of the input tensor.
struct Relu {
  // The id of input operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 input_operand_id;
  // The id of output operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 output_operand_id;
};

// Alters the shape of input operand to the output operand. This operation does
// not copy or change the content of the input, it just changes the tensorâ€™s
// logical dimensions.
struct Reshape {
  // The id of input operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 input_operand_id;
  // The id of output operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 output_operand_id;
};

// Corresponds to `MLOperand sigmoid(MLOperand x)` that compute the sigmoid
// function of the input tensor following the expression 1 / (exp(-x) + 1).
struct Sigmoid {
  // The id of input operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 input_operand_id;
  // The id of output operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 output_operand_id;
};

// Corresponds to `MLOperand softmax(MLOperand x)` that compute the softmax
// values of the 2-D input tensor along axis 1.
struct Softmax {
  // The id of input operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 input_operand_id;
  // The id of output operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 output_operand_id;
};

// Represents a split operation that splits an input tensor into multiple
// output tensors. The shape of the outputs and the specified axis determine
// how the split will be performed. Since axis specifies which input dimension
// will be split, the sum of all output dimension sizes along the axis
// dimension must be equal to the input tensorâ€™s axis dimension.
// Example:
//  input =     [1, 2, 3, (4)]
//  axis = 3
//  output[0] = [1, 2, 3, (1)]
//  output[1] = [1, 2, 3, (2)]
//  output[2] = [1, 2, 3, (1)]
struct Split {
  // The id of input operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 input_operand_id;
  // The ids of output operands used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  array<uint64> output_operand_ids;

  // Axis specifies which input tensor dimension will be split.
  uint32 axis = 0;
};

// Corresponds to `MLOperand tanh(MLOperand x)` that compute the hyperbolic
//tangent function of the input tensor following the expression
// (exp(2 * x) - 1) / (exp(2 * x) + 1).
struct Tanh {
  // The id of input operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 input_operand_id;
  // The id of output operand is used to get the `Operand` description from
  // `GraphInfo.id_to_operand_map`.
  uint64 output_operand_id;
};

// Represents the transpose operation that permutes the dimensions of the
// input tensor following the given permutation.
struct Transpose {
  // The id of input operand.
  uint64 input_operand_id;
  // The id of output operand.
  uint64 output_operand_id;
  // The values used to permute the dimensions of the input tensor.
  array<uint32> permutation;
};

// Resample the tensor values from the source to the destination spatial
// dimensions.
struct Resample2d {
  // The id of input operand.
  uint64 input_operand_id;
  // The id of output operand.
  uint64 output_operand_id;

  enum InterpolationMode {
    kNearestNeighbor,
    kLinear,
  };

  InterpolationMode mode;
};

// Represents the `MLActivation` which describes an activation function type
// to create other operations (Conv2d, BatchNormalization, for example).
union Activation {
  Clamp clamp;
  Relu relu;
  Sigmoid sigmoid;
  Softmax softmax;
  Tanh tanh;
};

// Holds one of operator.
union Operation {
  // Keep the order as the same as build methods of `MLGraphBuilder`.
  Clamp clamp;
  Concat concat;
  Conv2d conv2d;
  ElementWiseBinary element_wise_binary;
  Gemm gemm;
  Pad pad;
  Pool2d pool2d;
  Prelu prelu;
  Relu relu;
  Resample2d resample2d;
  Reshape reshape;
  Sigmoid sigmoid;
  Slice slice;
  Softmax softmax;
  Split split;
  Tanh tanh;
  Transpose transpose;
};

// Describes an entire WebNN graph information.
struct GraphInfo {
  // A map of all operands used in this `GraphInfo`, the key is the operand id.
  map<uint64, Operand> id_to_operand_map;
  // The id array from the `GraphInfo.id_to_operand_map` is used to identify the
  // input operands of this graph.
  array<uint64> input_operands;
  // The id array from the `GraphInfo.id_to_operand_map` is used to identify the
  // output operands of this graph.
  array<uint64> output_operands;
  // The operations are sorted in the topological order.
  array<Operation> operations;
  // The constant weight data specified through the MLGraphBuilder.constant()
  // method defined in the WebIDL, the key is the constant operand id.
  map<uint64, mojo_base.mojom.BigBuffer> constant_id_to_buffer_map;
};

enum ComputeResult {
  kOk,
  // The input for inference is invalid.
  kInvalidInputs,
  // Something has gone wrong during execution of the graph.
  kUnknownError,
};

// WebNNGraph runs in the GPU process and is called by the renderer process to
// execute the computational graph. Graph execution is performed by calling
// hardware accelerated OS machine learning APIs.
interface WebNNGraph {
  // Called by the renderer process to carry out the computational workload of
  // the compiled graph. The key of map is the name of input/output to identify
  // the tensor in the graph, the value is the shared memory to reduce memory
  // copy for inference. The Sync version of this method will only be called
  // from a worker thread.
  //
  // TODO(crbug.com/1488162): Remove [Sync] the standard groups decides that
  // the WebNN sync APIs are not required.
  [Sync]
  Compute(map<string, mojo_base.mojom.BigBuffer>  named_inputs)
      => (ComputeResult result,
          map<string, mojo_base.mojom.BigBuffer>? named_outputs);
};
