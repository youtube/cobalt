// Copyright 2024 The Chromium Authors
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

module blink.mojom;

// The status of the ModelStreamingResponder response.
// TODO(leimy): return more information about the erroneous case.
// These values are persisted to logs. Entries should not be renumbered and
// numeric values should never be reused.
//
// LINT.IfChange(ModelStreamingResponseStatus)
enum ModelStreamingResponseStatus {
  // There response is not fully streamed back yet, and the interface will
  // remain open.
  kOngoing = 0,
  // The ModelStreamingResponder completes and closes.
  kComplete = 1,
  // The following enums are for the case when the ModelStreamingResponder closes with some errors.
  kErrorUnknown = 2,
  // The request was invalid.
  kErrorInvalidRequest = 3,
  // The request was throttled.
  kErrorRequestThrottled = 4,
  // User permission errors such as not signed-in or not allowed to execute
  // model.
  kErrorPermissionDenied = 5,
  // Other generic failures.
  kErrorGenericFailure = 6,
  // Retryable error occurred in server.
  kErrorRetryableError = 7,
  // Non-retryable error occurred in server.
  kErrorNonRetryableError = 8,
  // Unsupported language.
  kErrorUnsupportedLanguage = 9,
  // Request was filtered.
  kErrorFiltered = 10,
  // Response was disabled.
  kErrorDisabled = 11,
  // The request was cancelled.
  kErrorCancelled = 12,
  // The session has been destroyed.
  kErrorSessionDestroyed = 13,
  // Append new items here.
};
// LINT.ThenChange(//tools/metrics/histograms/metadata/ai/enums.xml:AIModelStreamingResponseStatus)

// The struct stores the information about the context for the session.
struct ModelExecutionContextInfo {
  // The latest number of existing tokens.
  uint64 current_tokens;
  // Whether the context overflow happened in this round of model execution.
  bool did_overflow;
};

// The responder provides methods for the session to return the execution
// response in a streaming manner.
interface ModelStreamingResponder {
  // Called when the model execution encounters errors.
  OnError(ModelStreamingResponseStatus status);
  // Called when there is a new chunk of data available for streaming.
  // The data will be stored in `text`.
  OnStreaming(string text);
  // Called when the model execution completes. The optional `context_info`
  // contain information about the session like the current number of tokens
  // and if the context overflows.
  // It will only be set when necessary, for example, in the `AIAssistant`
  // interface as it maintains the context of the past model execution.
  OnCompletion(ModelExecutionContextInfo? context_info);
};
